2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="ipc.NewClient: af39fda9-DockerdPKG -> /run/host-services/backend.sock BackendAPI "
goroutine 1 [running, locked to thread]:
common/pkg/backend.NewClientForPath(...)
	common/pkg/backend/client.go:52
common/pkg/backend.NewClient(...)
	common/pkg/backend/client.go:46
linuxkit/pkg/desktop-host-tools/pkg/dockerd.init()
	linuxkit/pkg/desktop-host-tools/pkg/dockerd/start.go:29 +0x85
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="ipc.NewClient: 4d88ceed-DockerdPKG -> /run/host-services/gui-api.sock  "
goroutine 1 [running, locked to thread]:
common/pkg/gui.NewClientForPath(0xc37dbe, 0xa, 0xc4d086, 0x1f, 0xc37d14, 0xa)
	common/pkg/gui/client.go:82 +0x57
common/pkg/gui.NewClient(...)
	common/pkg/gui/client.go:69
linuxkit/pkg/desktop-host-tools/pkg/dockerd.init()
	linuxkit/pkg/desktop-host-tools/pkg/dockerd/start.go:30 +0x11b
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="mutagen: performing initial housekeeping"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="mutagen: waiting for connections"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="mutagen agent started in the background"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Starting FUSE client"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="connecting to filesystem server on vsock:4099"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="listening for mount requests on unix:/run/grpcfuse.mount.sock"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="listening for mount/unmount requests"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="listening for inotify event injections on unix:/run/guest-services/filesystem-event.sock"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="listening for test requests on unix:/run/guest-services/filesystem-test.sock"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Starting containerd"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="checking the permissions of files in /dev are 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/cachefiles 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/console 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/cpu_dma_latency 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/cuse 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/fd 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/full 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/fuse 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/hpet 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/hwrng 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/kcore 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/kmsg 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop-control 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop2 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop3 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop4 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop5 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop6 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/loop7 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/mem 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd10 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd11 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd12 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd13 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd14 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd15 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd2 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd3 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd4 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd5 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd6 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd7 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd8 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nbd9 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/null 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/nvram 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/port 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/psaux 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ptmx 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram10 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram11 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram12 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram13 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram14 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram15 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram2 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram3 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram4 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram5 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram6 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram7 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram8 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ram9 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/random 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/rtc0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/stderr 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/stdin 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/stdout 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty10 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty11 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty12 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty13 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty14 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty15 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty16 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty17 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty18 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty19 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty2 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty20 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty21 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty22 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty23 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty24 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty25 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty26 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty27 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty28 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty29 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty3 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty30 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty31 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty32 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty33 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty34 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty35 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty36 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty37 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty38 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty39 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty4 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty40 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty41 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty42 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty43 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty44 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty45 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty46 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty47 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty48 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty49 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty5 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty50 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty51 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty52 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty53 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty54 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty55 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty56 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty57 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty58 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty59 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty6 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty60 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty61 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty62 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty63 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty7 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty8 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/tty9 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ttyS0 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ttyS1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ttyS2 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/ttyS3 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/uinput 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/urandom 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcs 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Starting lifecycle-server"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -N desktop"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcs1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcsa 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcsa1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcsu 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vcsu1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vda 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vda1 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/vsock 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="chmod /dev/zero 666"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=error msg="permissions of /dev are incorrect: 1 error occurred:\n\t* chmod /dev/fd 666: chmod /dev/fd: operation not permitted\n\n"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Setting up certificates"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Increasing resource limits"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Starting docker engine"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="waiting for the backend to respond"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="(c23e66c8) af39fda9-DockerdPKG C->S BackendAPI GET /ping"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -A PREROUTING -j desktop"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="(c23e66c8) af39fda9-DockerdPKG C<-S 7deddf34-VMAPI GET /ping (1.424597ms): {\"serverTime\":1631580944139064000}"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="backend is responding"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="mounting host fileserver"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="hypervisor:hyperkit"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="create symlinks"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="try to mount host fileserver"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /ping"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 10.96.0.0/12 -j RETURN"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="ipc.NewServer: eb18ad98-VMDockerdAPI <- /run/guest-services/lifecycle-server.sock"
linuxkit/pkg/desktop-host-tools/pkg/server.(*server).Start(0xc00051bf68, 0x4, 0xc000566748)
	linuxkit/pkg/desktop-host-tools/pkg/server/server.go:49 +0x105
main.run.func1(0xf50938, 0xc00018e480, 0xf3fe88, 0xc000424048, 0x0, 0xc000522ae0)
	linuxkit/pkg/desktop-host-tools/cmd/lifecycle-server/main.go:101 +0x108
created by main.run
	linuxkit/pkg/desktop-host-tools/cmd/lifecycle-server/main.go:98 +0x414
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /vm/resume"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /vm/shutdown"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /vm/httpproxy"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /kubernetes/start"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /kubernetes/stop"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /kubernetes/reset"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /kubernetes/ensure-controllers"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /mutagen/cleanup"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /docker/reload"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /docker"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /fuse/time"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /fuse/count"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /fuse/filename"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /fuse/calls"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /fuse/follow"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /mutagen/info"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /volume/approve"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /profile/start"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring GET /profile/stop"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /fuse/reset"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="eb18ad98-VMDockerdAPI : registring POST /hub-login"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="starting eb18ad98-VMDockerdAPI for component lifecycle-server-pkg on /run/guest-services/lifecycle-server.sock"
2021-09-14T00:55:43Z lifecycle-server 2021-09-14T00:55:43.933343Z docker-desktop lifecycle-server info: "mount server is on unix:/run/grpcfuse.mount.sock"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 10.1.0.0/16 -j RETURN"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="attempting to mount filesystem / with ID 2 on /host_mnt"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="kernel entry timeout: 1h0m0s"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="kernel attribute timeout: 1h0m0s"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="kernel negative timeout: 0s"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 192.168.65.0/24 -j RETURN"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="Filesystem mounted to root /host_mnt"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="shutting down any old Docker engine"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="waiting for the clock to be set"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="clock value looks plausible: 2021-09-14 00:55:43.943798776 +0000 UTC m=+0.165063908"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="fetching daemon.json from the host"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="(58726dab) af39fda9-DockerdPKG C->S BackendAPI GET /engine/daemon.json"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 192.168.65.5/32 -j RETURN"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="(58726dab) af39fda9-DockerdPKG C<-S 7deddf34-VMAPI GET /engine/daemon.json (2.119406ms): {\"builder\":{\"gc\":{\"defaultKeepStorage\":\"20GB\",\"enabled\":true}},\"experimental\":false,\"features\":{\"buildkit\":true}}"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="adding hosts to daemon.json"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="writing daemon.json"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="no metrics-addr in daemon.json: not exposing a metrics port"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="waiting for internal network to respond"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 127.0.0.0/8 -j RETURN"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="detected DNS gateway.docker.internal -> [192.168.65.2]"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="reading HTTP proxy configuration"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="creating /run/xtables.lock file"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="waiting for Docker API proxy socket"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="resolving host.docker.internal. DNS"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="launching docker engine"
2021-09-14T00:55:43Z lifecycle-server time="2021-09-14T00:55:43Z" level=info msg="waiting for Docker API to respond"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="mutagen: connection received"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 172.17.0.0/16 -j RETURN"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="docker API is ready"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="sending docker state running to backend"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(c29a42de) af39fda9-DockerdPKG C->S BackendAPI POST /events: {\"timestamp\":1631580944494612828,\"HasServerTimestamp\":false,\"content\":\"lifecycle-server sent docker state running\",\"docker\":\"running\"}"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(c29a42de) af39fda9-DockerdPKG C<-S 7deddf34-VMAPI POST /events (13.426639ms): OK"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(44e25539b0-59) eb18ad98-VMDockerdAPI S<-C 00678d97-DockerHubHDL GET /ping"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).NewLoginInfo(0xc000658ac0, 0xc000824270, 0x16, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:168 +0xed
common/cmd/com.docker.backend/internal/handlers.(*dockerHub).updateProxies(0x68048e0, 0x5b9f528, 0xc000466380, 0xc000824270, 0x16, 0x0, 0x0, 0x0, 0x0, 0x0)
	common/cmd/com.docker.backend/internal/handlers/dockerhub.go:87 +0x26a
common/cmd/com.docker.backend/internal/handlers.(*dockerHub).NewLoginInfo
	/Users/administrator/jenkins/workspace/sktop_desktop-build_master-merge/src/github.com/doc
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(44e25539b0-59) eb18ad98-VMDockerdAPI S->C 00678d97-DockerHubHDL GET /ping (144.991µs): {\"serverTime\":1631580944780120148}"
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(44e25539b0-60) eb18ad98-VMDockerdAPI S<-C 00678d97-DockerHubHDL POST /hub-login"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).NewLoginInfo(0xc000658ac0, 0xc000824270, 0x16, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:168 +0xed
common/cmd/com.docker.backend/internal/handlers.(*dockerHub).updateProxies(0x68048e0, 0x5b9f528, 0xc000466380, 0xc000824270, 0x16, 0x0, 0x0, 0x0, 0x0, 0x0)
	common/cmd/com.docker.backend/internal/handlers/dockerhub.go:87 +0x26a
common/cmd/com.docker.backend/internal/handlers.(*dockerHub).NewLoginInfo
	/Users/administrator/jenkins/workspace/sktop_desktop-build_master-merge/src/github.c
2021-09-14T00:55:44Z lifecycle-server time="2021-09-14T00:55:44Z" level=info msg="(44e25539b0-60) eb18ad98-VMDockerdAPI S->C 00678d97-DockerHubHDL POST /hub-login (195.281µs): OK"
2021-09-14T00:56:13Z lifecycle-server time="2021-09-14T00:56:13Z" level=info msg="(730f1115) eb18ad98-VMDockerdAPI S<-C 33c5ba76-DriverCMD GET /ping"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).Ping(0xc0000aa7f0, 0xb72279b31, 0x50edd00)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:74 +0x33
main.handleSystemCalls(0x4b80610, 0xc0000aa7f0)
	mac/backend/cmd/com.docker.driver.amd64-linux/main.go:504 +0x2c7
created by main.run
	mac/backend/cmd/com.docker.driver.amd64-linux/main.go:94 +0x237
2021-09-14T00:56:13Z lifecycle-server time="2021-09-14T00:56:13Z" level=info msg="(730f1115) eb18ad98-VMDockerdAPI S->C 33c5ba76-DriverCMD GET /ping (511.343µs): {\"serverTime\":1631580973910756673}"
2021-09-14T00:56:13Z lifecycle-server time="2021-09-14T00:56:13Z" level=info msg="(2b0ce3d9) eb18ad98-VMDockerdAPI S<-C 33c5ba76-DriverCMD POST /vm/shutdown"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).VMShutdown(0xc0000aa7f0, 0x4, 0xc00067bf10)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:96 +0x64
main.handleSystemCalls(0x4b80610, 0xc0000aa7f0)
	mac/backend/cmd/com.docker.driver.amd64-linux/main.go:519 +0x564
created by main.run
	mac/backend/cmd/com.docker.driver.amd64-linux/main.go:94 +0x237
2021-09-14T00:56:13Z lifecycle-server time="2021-09-14T00:56:13Z" level=info msg="(2b0ce3d9) eb18ad98-VMDockerdAPI S->C 33c5ba76-DriverCMD POST /vm/shutdown (8.996669ms): OK"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="ipc.NewClient: 5160cdb7-DockerdPKG -> /run/host-services/backend.sock BackendAPI "
goroutine 1 [running, locked to thread]:
common/pkg/backend.NewClientForPath(...)
	common/pkg/backend/client.go:52
common/pkg/backend.NewClient(...)
	common/pkg/backend/client.go:46
linuxkit/pkg/desktop-host-tools/pkg/dockerd.init()
	linuxkit/pkg/desktop-host-tools/pkg/dockerd/start.go:29 +0x85
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="ipc.NewClient: 06d11d0c-DockerdPKG -> /run/host-services/gui-api.sock  "
goroutine 1 [running, locked to thread]:
common/pkg/gui.NewClientForPath(0xc37dbe, 0xa, 0xc4d086, 0x1f, 0xc37d14, 0xa)
	common/pkg/gui/client.go:82 +0x57
common/pkg/gui.NewClient(...)
	common/pkg/gui/client.go:69
linuxkit/pkg/desktop-host-tools/pkg/dockerd.init()
	linuxkit/pkg/desktop-host-tools/pkg/dockerd/start.go:30 +0x11b
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="mutagen: waiting for connections"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="mutagen: performing initial housekeeping"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="mutagen agent started in the background"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Starting FUSE client"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="connecting to filesystem server on vsock:4099"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="listening for mount requests on unix:/run/grpcfuse.mount.sock"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="listening for mount/unmount requests"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="listening for inotify event injections on unix:/run/guest-services/filesystem-event.sock"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="listening for test requests on unix:/run/guest-services/filesystem-test.sock"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Starting containerd"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="checking the permissions of files in /dev are 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Starting lifecycle-server"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/fd 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/kcore 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/stderr 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/stdin 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/stdout 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/ttyS0 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="chmod /dev/ttyS1 666"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=error msg="permissions of /dev are incorrect: 1 error occurred:\n\t* chmod /dev/fd 666: chmod /dev/fd: operation not permitted\n\n"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Setting up certificates"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -N desktop"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Increasing resource limits"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Starting docker engine"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="waiting for the backend to respond"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="(66460f82) 5160cdb7-DockerdPKG C->S BackendAPI GET /ping"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="(66460f82) 5160cdb7-DockerdPKG C<-S 7deddf34-VMAPI GET /ping (1.581398ms): {\"serverTime\":1631580990608573000}"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="backend is responding"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="mounting host fileserver"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="hypervisor:hyperkit"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="create symlinks"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="try to mount host fileserver"
2021-09-14T00:56:29Z lifecycle-server 2021-09-14T00:56:29.781466Z docker-desktop lifecycle-server info: "mount server is on unix:/run/grpcfuse.mount.sock"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="attempting to mount filesystem / with ID 2 on /host_mnt"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="kernel entry timeout: 1h0m0s"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="kernel attribute timeout: 1h0m0s"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="kernel negative timeout: 0s"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -A PREROUTING -j desktop"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="Filesystem mounted to root /host_mnt"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="shutting down any old Docker engine"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="waiting for the clock to be set"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="clock value looks plausible: 2021-09-14 00:56:29.792174011 +0000 UTC m=+0.186683656"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="fetching daemon.json from the host"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="(b6a2ecb0) 5160cdb7-DockerdPKG C->S BackendAPI GET /engine/daemon.json"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /ping"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="ipc.NewServer: 65bee725-VMDockerdAPI <- /run/guest-services/lifecycle-server.sock"
linuxkit/pkg/desktop-host-tools/pkg/server.(*server).Start(0xc0002e9f68, 0x200000004, 0xc0002f8f48)
	linuxkit/pkg/desktop-host-tools/pkg/server/server.go:49 +0x105
main.run.func1(0xf50938, 0xc000196600, 0xf3fe88, 0xc00031e058, 0x0, 0xc00007e8a0)
	linuxkit/pkg/desktop-host-tools/cmd/lifecycle-server/main.go:101 +0x108
created by main.run
	linuxkit/pkg/desktop-host-tools/cmd/lifecycle-server/main.go:98 +0x414
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /vm/resume"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /vm/shutdown"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 10.96.0.0/12 -j RETURN"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /vm/httpproxy"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /kubernetes/start"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /kubernetes/stop"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /kubernetes/reset"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /kubernetes/ensure-controllers"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /mutagen/cleanup"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /docker/reload"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /docker"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /fuse/time"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /fuse/count"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /fuse/filename"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /fuse/calls"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /fuse/follow"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /mutagen/info"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /volume/approve"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /profile/start"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring GET /profile/stop"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /fuse/reset"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="65bee725-VMDockerdAPI : registring POST /hub-login"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="starting 65bee725-VMDockerdAPI for component lifecycle-server-pkg on /run/guest-services/lifecycle-server.sock"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 10.1.0.0/16 -j RETURN"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="(b6a2ecb0) 5160cdb7-DockerdPKG C<-S 7deddf34-VMAPI GET /engine/daemon.json (4.303324ms): {\"builder\":{\"gc\":{\"defaultKeepStorage\":\"20GB\",\"enabled\":true}},\"experimental\":false,\"features\":{\"buildkit\":true}}"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="adding hosts to daemon.json"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="writing daemon.json"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="no metrics-addr in daemon.json: not exposing a metrics port"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="waiting for internal network to respond"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 192.168.65.0/24 -j RETURN"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="detected DNS gateway.docker.internal -> [192.168.65.2]"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="reading HTTP proxy configuration"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="creating /run/xtables.lock file"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="waiting for Docker API proxy socket"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="resolving host.docker.internal. DNS"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 192.168.65.5/32 -j RETURN"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="launching docker engine"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 127.0.0.0/8 -j RETURN"
2021-09-14T00:56:29Z lifecycle-server time="2021-09-14T00:56:29Z" level=info msg="waiting for Docker API to respond"
2021-09-14T00:56:30Z lifecycle-server time="2021-09-14T00:56:30Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 172.17.0.0/16 -j RETURN"
2021-09-14T00:56:30Z lifecycle-server time="2021-09-14T00:56:30Z" level=info msg="mutagen: connection received"
2021-09-14T00:56:31Z lifecycle-server time="2021-09-14T00:56:31Z" level=info msg="docker API is ready"
2021-09-14T00:56:31Z lifecycle-server time="2021-09-14T00:56:31Z" level=info msg="sending docker state running to backend"
2021-09-14T00:56:31Z lifecycle-server time="2021-09-14T00:56:31Z" level=info msg="(ade5dc08) 5160cdb7-DockerdPKG C->S BackendAPI POST /events: {\"timestamp\":1631580991046822929,\"HasServerTimestamp\":false,\"content\":\"lifecycle-server sent docker state running\",\"docker\":\"running\"}"
2021-09-14T00:56:31Z lifecycle-server time="2021-09-14T00:56:31Z" level=info msg="(ade5dc08) 5160cdb7-DockerdPKG C<-S 7deddf34-VMAPI POST /events (52.938253ms): OK"
2021-09-14T00:56:32Z lifecycle-server time="2021-09-14T00:56:32Z" level=info msg="(170c17bb) 65bee725-VMDockerdAPI S<-C 51fa1bcd-KubernetesHDL POST /kubernetes/start"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).StartKubernetes(0xc0006596d0, 0xc0005040c0, 0x0, 0x0)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:117 +0x79
common/cmd/com.docker.backend/internal/kubernetes.(*Manager).doStart(0xc000290ba0, 0x5b95001, 0x5b9f528, 0xc0004662a0, 0x43e5b60, 0xc000698120)
	common/cmd/com.docker.backend/internal/kubernetes/bootstrap.go:160 +0x109
common/cmd/com.docker.backend/internal/kubernetes.(*Manager).Start(0xc000290ba0, 0x5b9f528, 0xc0004662a0, 0x0, 0x0)
	/Users/administrator/jenkins/workspace/sktop_desktop-build_master-merge/src/github.com/
2021-09-14T00:56:32Z lifecycle-server time="2021-09-14T00:56:32Z" level=info msg="(170c17bb) 65bee725-VMDockerdAPI S<-C 51fa1bcd-KubernetesHDL bind: {\"apiserver-etcd-client.crt\":\"-----BEGIN CERTIFICATE-----\\nMIIDKDCCAhCgAwIBAgIIQE62zqbNtsEwDQYJKoZIhvcNAQELBQAwEjEQMA4GA1UE\\nAxMHZXRjZC1jYTAeFw0yMTA5MTQwMDU2MTRaFw0yMjA5MTQwMDU2MTRaMD4xFzAV\\nBgNVBAoTDnN5c3RlbTptYXN0ZXJzMSMwIQYDVQQDExprdWJlLWFwaXNlcnZlci1l\\ndGNkLWNsaWVudDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMri7Oyt\\nwpAT58NzSHUFNFE/+Yzyb8KkzRPg7tlBmr6kb/a9ujQXUJlhCj5c8SWrjb7fbyqk\\nxVdyj67/MUYtyJBpTULaPToGLVJeLlb8q2Z/PxzHIanaoCGHpCOjDUt3nO2u1rYZ\\nEjVGZXAdlPxt90XTUk82qSVTnFHGNfwbIx2wWZdm0YN1+/tfpfQobRnjwlpDQChI\\nTWOXR+DLiecHh/v+eZDsxDWtAIcAXVzvWm9n851XnLdquKFkY+TpsAA5J5P0zmcg\\n9P1P77CUP8b4KbJHXk1Iu+Vl1nVr+xvGgrVxOgvjPKE87B18BaKL+RKh0U6tgQXJ\\nCrprV4MrAQW4uk8CAwEAAaNWMFQwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\\nCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAU8UNxgFfa6FTidbbR\\nw/ZY5/H/JKkwDQYJKoZIhvcNAQELBQADggEBAD0JzHp/5kwuzjbdfu5L/lESLOKv\\n10+WDY/m9Q9owqAMGXYS0KhP/2DJGJaqi/9bSpY9E0xvzQ
2021-09-14T00:56:32Z lifecycle-server + '[' -f /run/config/kubelet/enabled ']'
2021-09-14T00:56:32Z lifecycle-server + kube-pull.sh
2021-09-14T00:56:32Z lifecycle-server + KUBERNETES=docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:32Z lifecycle-server + CACHEDIR=/var/lib/kube-binary-cache-debian
2021-09-14T00:56:32Z lifecycle-server + echo 'kube-pull.sh: checking kubernetes binary cache'
2021-09-14T00:56:32Z lifecycle-server kube-pull.sh: checking kubernetes binary cache
2021-09-14T00:56:32Z lifecycle-server + mkdir -p /var/lib/kube-binary-cache-debian
2021-09-14T00:56:32Z lifecycle-server + docker image inspect docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:32Z lifecycle-server + echo 'docker pull docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian'
2021-09-14T00:56:32Z lifecycle-server docker pull docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:32Z lifecycle-server + docker pull docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:33Z lifecycle-server kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian: Pulling from docker/desktop-kubernetes
2021-09-14T00:56:33Z lifecycle-server cbef3821202f: Pulling fs layer
2021-09-14T00:56:33Z lifecycle-server 14eea56abe04: Pulling fs layer
2021-09-14T00:56:33Z lifecycle-server c51336377b5a: Pulling fs layer
2021-09-14T00:56:37Z lifecycle-server time="2021-09-14T00:56:37Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:56:38Z lifecycle-server cbef3821202f: Verifying Checksum
2021-09-14T00:56:38Z lifecycle-server cbef3821202f: Download complete
2021-09-14T00:56:39Z lifecycle-server cbef3821202f: Pull complete
2021-09-14T00:56:40Z lifecycle-server 14eea56abe04: Verifying Checksum
2021-09-14T00:56:40Z lifecycle-server 14eea56abe04: Download complete
2021-09-14T00:56:41Z lifecycle-server 14eea56abe04: Pull complete
2021-09-14T00:56:42Z lifecycle-server time="2021-09-14T00:56:42Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:56:43Z lifecycle-server c51336377b5a: Download complete
2021-09-14T00:56:45Z lifecycle-server c51336377b5a: Pull complete
2021-09-14T00:56:45Z lifecycle-server Digest: sha256:e1629e35ab2ffbf1dc5582bab121d2fd6b0d40cfe36cb493c83772d906ed71a0
2021-09-14T00:56:45Z lifecycle-server Status: Downloaded newer image for docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:45Z lifecycle-server docker.io/docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:45Z lifecycle-server ++ docker create docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:45Z lifecycle-server + kubernetes=e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea
2021-09-14T00:56:45Z lifecycle-server + trap finish EXIT
2021-09-14T00:56:45Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:45Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubelet ']'
2021-09-14T00:56:45Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/kubelet /var/lib/kube-binary-cache-debian/kubelet
2021-09-14T00:56:46Z lifecycle-server + rm -f /usr/bin/kubelet
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /usr/bin/kubelet ']'
2021-09-14T00:56:46Z lifecycle-server + ln -s /var/lib/kube-binary-cache-debian/kubelet /usr/bin/kubelet
2021-09-14T00:56:46Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubeadm ']'
2021-09-14T00:56:46Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/kubeadm /var/lib/kube-binary-cache-debian/kubeadm
2021-09-14T00:56:46Z lifecycle-server + rm -f /usr/bin/kubeadm
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /usr/bin/kubeadm ']'
2021-09-14T00:56:46Z lifecycle-server + ln -s /var/lib/kube-binary-cache-debian/kubeadm /usr/bin/kubeadm
2021-09-14T00:56:46Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubectl ']'
2021-09-14T00:56:46Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/kubectl /var/lib/kube-binary-cache-debian/kubectl
2021-09-14T00:56:46Z lifecycle-server + rm -f /usr/bin/kubectl
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /usr/bin/kubectl ']'
2021-09-14T00:56:46Z lifecycle-server + ln -s /var/lib/kube-binary-cache-debian/kubectl /usr/bin/kubectl
2021-09-14T00:56:46Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/crictl ']'
2021-09-14T00:56:46Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/crictl /var/lib/kube-binary-cache-debian/crictl
2021-09-14T00:56:46Z lifecycle-server + rm -f /usr/bin/crictl
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /usr/bin/crictl ']'
2021-09-14T00:56:46Z lifecycle-server + ln -s /var/lib/kube-binary-cache-debian/crictl /usr/bin/crictl
2021-09-14T00:56:46Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:46Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/critest ']'
2021-09-14T00:56:46Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/critest /var/lib/kube-binary-cache-debian/critest
2021-09-14T00:56:47Z lifecycle-server + rm -f /usr/bin/critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/critest ']'
2021-09-14T00:56:47Z lifecycle-server + ln -s /var/lib/kube-binary-cache-debian/critest /usr/bin/critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/cni.tgz ']'
2021-09-14T00:56:47Z lifecycle-server + docker cp e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea:/binaries/cni.tgz /var/lib/kube-binary-cache-debian/cni.tgz
2021-09-14T00:56:47Z lifecycle-server + finish
2021-09-14T00:56:47Z lifecycle-server + docker rm e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea
2021-09-14T00:56:47Z lifecycle-server e5776586812ec143ff233db3cc0098fc48a85845149dd84a0064afab0f0570ea
2021-09-14T00:56:47Z lifecycle-server + echo 'kube-start.sh: starting the cluster.'
2021-09-14T00:56:47Z lifecycle-server kube-start.sh: starting the cluster.
2021-09-14T00:56:47Z lifecycle-server + rm -f /run/config/kubelet/disabled
2021-09-14T00:56:47Z lifecycle-server + touch /run/config/kubelet/enabled
2021-09-14T00:56:47Z lifecycle-server + /usr/bin/logwrite -n kubelet kubelet.sh
2021-09-14T00:56:47Z lifecycle-server + set -euxo pipefail
2021-09-14T00:56:47Z lifecycle-server + exec
2021-09-14T00:56:47Z lifecycle-server + CACHEDIR=/var/lib/kube-binary-cache-debian
2021-09-14T00:56:47Z lifecycle-server + configFile=/etc/kubernetes/kubelet.conf
2021-09-14T00:56:47Z lifecycle-server + currentVersionFile=/var/lib/kubeadm/current-version
2021-09-14T00:56:47Z lifecycle-server + '[' -f /run/config/kubelet/disabled ']'
2021-09-14T00:56:47Z lifecycle-server + kube-pull.sh
2021-09-14T00:56:47Z lifecycle-server + KUBERNETES=docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:47Z lifecycle-server + CACHEDIR=/var/lib/kube-binary-cache-debian
2021-09-14T00:56:47Z lifecycle-server + echo 'kube-pull.sh: checking kubernetes binary cache'
2021-09-14T00:56:47Z lifecycle-server kube-pull.sh: checking kubernetes binary cache
2021-09-14T00:56:47Z lifecycle-server + mkdir -p /var/lib/kube-binary-cache-debian
2021-09-14T00:56:47Z lifecycle-server + docker image inspect docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:47Z lifecycle-server ++ docker create docker/desktop-kubernetes:kubernetes-v1.21.4-cni-v0.8.5-critools-v1.17.0-debian
2021-09-14T00:56:47Z lifecycle-server time="2021-09-14T00:56:47Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:56:47Z lifecycle-server + kubernetes=1d4d4fcf91ece3082fa650d25978a87fee6129955e1e778ae77379701b077720
2021-09-14T00:56:47Z lifecycle-server + trap finish EXIT
2021-09-14T00:56:47Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubelet ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/kubelet ']'
2021-09-14T00:56:47Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubeadm ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/kubeadm ']'
2021-09-14T00:56:47Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/kubectl ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/kubectl ']'
2021-09-14T00:56:47Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/crictl ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/crictl ']'
2021-09-14T00:56:47Z lifecycle-server + for BINARY in kubelet kubeadm kubectl crictl critest
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/critest ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /usr/bin/critest ']'
2021-09-14T00:56:47Z lifecycle-server + '[' '!' -e /var/lib/kube-binary-cache-debian/cni.tgz ']'
2021-09-14T00:56:47Z lifecycle-server + finish
2021-09-14T00:56:47Z lifecycle-server + docker rm 1d4d4fcf91ece3082fa650d25978a87fee6129955e1e778ae77379701b077720
2021-09-14T00:56:47Z lifecycle-server 1d4d4fcf91ece3082fa650d25978a87fee6129955e1e778ae77379701b077720
2021-09-14T00:56:47Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:47Z lifecycle-server + mkdir -p /var/lib/cni-plugins/bin
2021-09-14T00:56:47Z lifecycle-server + tar -xzf /var/lib/kube-binary-cache-debian/cni.tgz -C /var/lib/cni-plugins/bin
2021-09-14T00:56:48Z lifecycle-server + '[' -d /etc/kube-images ']'
2021-09-14T00:56:48Z lifecycle-server + for entry in /etc/kube-images/*.tar
2021-09-14T00:56:48Z lifecycle-server + [[ -e /etc/kube-images/*.tar ]]
2021-09-14T00:56:48Z lifecycle-server + break
2021-09-14T00:56:48Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:48Z lifecycle-server + echo 'kubelet.sh: cluster init required'
2021-09-14T00:56:48Z lifecycle-server kubelet.sh: cluster init required
2021-09-14T00:56:48Z lifecycle-server + HOSTNAME=docker-desktop
2021-09-14T00:56:48Z lifecycle-server + grep /etc/kubernetes/kubelet.conf system:node:docker-for-desktop
2021-09-14T00:56:48Z lifecycle-server + kubeadm-init.sh
2021-09-14T00:56:48Z lifecycle-server + echo 'kubelet.sh: waiting for /etc/kubernetes/kubelet.conf'
2021-09-14T00:56:48Z lifecycle-server kubelet.sh: waiting for /etc/kubernetes/kubelet.conf
2021-09-14T00:56:48Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:48Z lifecycle-server + sleep 1
2021-09-14T00:56:48Z lifecycle-server + touch /var/lib/kubeadm/.kubeadm-init.sh-started
2021-09-14T00:56:48Z lifecycle-server ++ getent hosts kubernetes.docker.internal
2021-09-14T00:56:48Z lifecycle-server ++ cut -f 1 -d ' '
2021-09-14T00:56:48Z lifecycle-server + IP=192.168.65.4
2021-09-14T00:56:48Z lifecycle-server + sed s/%ADVERTISEADDRESS%/192.168.65.4/ /etc/kubeadm/kubeadm.yaml.in
2021-09-14T00:56:48Z lifecycle-server + kubeadm init --ignore-preflight-errors=all --config /etc/kubeadm/kubeadm.yaml
2021-09-14T00:56:48Z lifecycle-server W0914 00:56:48.108595    2153 strict.go:54] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeproxy.config.k8s.io", Version:"v1alpha1", Kind:"KubeProxyConfiguration"}: error unmarshaling JSON: while decoding JSON: json: unknown field "resourceContainer"
2021-09-14T00:56:48Z lifecycle-server W0914 00:56:48.147635    2153 kubelet.go:223] cannot determine if systemd-resolved is active: no supported init system detected, skipping checking for services
2021-09-14T00:56:48Z lifecycle-server [init] Using Kubernetes version: v1.21.4
2021-09-14T00:56:48Z lifecycle-server [preflight] Running pre-flight checks
2021-09-14T00:56:48Z lifecycle-server 	[WARNING Firewalld]: no supported init system detected, skipping checking for services
2021-09-14T00:56:48Z lifecycle-server 	[WARNING Service-Docker]: no supported init system detected, skipping checking for services
2021-09-14T00:56:48Z lifecycle-server 	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
2021-09-14T00:56:48Z lifecycle-server 	[WARNING Swap]: running with swap on is not supported. Please disable swap
2021-09-14T00:56:48Z lifecycle-server 	[WARNING FileExisting-conntrack]: conntrack not found in system path
2021-09-14T00:56:48Z lifecycle-server 	[WARNING FileExisting-ethtool]: ethtool not found in system path
2021-09-14T00:56:48Z lifecycle-server 	[WARNING Service-Kubelet]: no supported init system detected, skipping checking for services
2021-09-14T00:56:48Z lifecycle-server [preflight] Pulling images required for setting up a Kubernetes cluster
2021-09-14T00:56:48Z lifecycle-server [preflight] This might take a minute or two, depending on the speed of your internet connection
2021-09-14T00:56:48Z lifecycle-server [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
2021-09-14T00:56:49Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:49Z lifecycle-server + sleep 1
2021-09-14T00:56:50Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:50Z lifecycle-server + sleep 1
2021-09-14T00:56:51Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:51Z lifecycle-server + sleep 1
2021-09-14T00:56:52Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:52Z lifecycle-server + sleep 1
2021-09-14T00:56:52Z lifecycle-server time="2021-09-14T00:56:52Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:56:53Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:53Z lifecycle-server + sleep 1
2021-09-14T00:56:54Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:54Z lifecycle-server + sleep 1
2021-09-14T00:56:55Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:55Z lifecycle-server + sleep 1
2021-09-14T00:56:56Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:56Z lifecycle-server + sleep 1
2021-09-14T00:56:57Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:57Z lifecycle-server + sleep 1
2021-09-14T00:56:57Z lifecycle-server time="2021-09-14T00:56:57Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:56:58Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:58Z lifecycle-server + sleep 1
2021-09-14T00:56:59Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:56:59Z lifecycle-server + sleep 1
2021-09-14T00:57:00Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:00Z lifecycle-server + sleep 1
2021-09-14T00:57:01Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:01Z lifecycle-server + sleep 1
2021-09-14T00:57:02Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:02Z lifecycle-server + sleep 1
2021-09-14T00:57:02Z lifecycle-server time="2021-09-14T00:57:02Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:03Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:03Z lifecycle-server + sleep 1
2021-09-14T00:57:04Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:04Z lifecycle-server + sleep 1
2021-09-14T00:57:05Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:05Z lifecycle-server + sleep 1
2021-09-14T00:57:06Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:06Z lifecycle-server + sleep 1
2021-09-14T00:57:07Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:07Z lifecycle-server + sleep 1
2021-09-14T00:57:07Z lifecycle-server time="2021-09-14T00:57:07Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:08Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:08Z lifecycle-server + sleep 1
2021-09-14T00:57:09Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:09Z lifecycle-server + sleep 1
2021-09-14T00:57:10Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:10Z lifecycle-server + sleep 1
2021-09-14T00:57:11Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:11Z lifecycle-server + sleep 1
2021-09-14T00:57:12Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:12Z lifecycle-server + sleep 1
2021-09-14T00:57:12Z lifecycle-server time="2021-09-14T00:57:12Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:13Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:13Z lifecycle-server + sleep 1
2021-09-14T00:57:14Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:14Z lifecycle-server + sleep 1
2021-09-14T00:57:15Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:15Z lifecycle-server + sleep 1
2021-09-14T00:57:16Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:16Z lifecycle-server + sleep 1
2021-09-14T00:57:17Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:17Z lifecycle-server + sleep 1
2021-09-14T00:57:17Z lifecycle-server time="2021-09-14T00:57:17Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:18Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:18Z lifecycle-server + sleep 1
2021-09-14T00:57:19Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:19Z lifecycle-server + sleep 1
2021-09-14T00:57:20Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:20Z lifecycle-server + sleep 1
2021-09-14T00:57:21Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:21Z lifecycle-server + sleep 1
2021-09-14T00:57:22Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:22Z lifecycle-server + sleep 1
2021-09-14T00:57:22Z lifecycle-server time="2021-09-14T00:57:22Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:22Z lifecycle-server [certs] Using certificateDir folder "/run/config/pki"
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing ca certificate authority
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing apiserver certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing apiserver-kubelet-client certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing front-proxy-ca certificate authority
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing front-proxy-client certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing etcd/ca certificate authority
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing etcd/server certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing etcd/peer certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing etcd/healthcheck-client certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using existing apiserver-etcd-client certificate and key on disk
2021-09-14T00:57:22Z lifecycle-server [certs] Using the existing "sa" key
2021-09-14T00:57:22Z lifecycle-server [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
2021-09-14T00:57:22Z lifecycle-server [kubeconfig] Writing "admin.conf" kubeconfig file
2021-09-14T00:57:23Z lifecycle-server [kubeconfig] Writing "kubelet.conf" kubeconfig file
2021-09-14T00:57:23Z lifecycle-server + '[' -f /etc/kubernetes/kubelet.conf ']'
2021-09-14T00:57:23Z lifecycle-server + echo 'kubelet.sh: /etc/kubernetes/kubelet.conf has arrived'
2021-09-14T00:57:23Z lifecycle-server kubelet.sh: /etc/kubernetes/kubelet.conf has arrived
2021-09-14T00:57:23Z lifecycle-server + mkdir -p /etc/kubernetes/manifests
2021-09-14T00:57:23Z lifecycle-server + exec kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --config /etc/kubeadm/kubelet.yaml --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/var/lib/cni-plugins/bin --kube-reserved-cgroup=podruntime --system-reserved-cgroup=systemreserved --cgroup-root=kubepods --hostname-override=docker-desktop
2021-09-14T00:57:23Z lifecycle-server Flag --network-plugin has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --cni-conf-dir has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --cni-bin-dir has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server Flag --network-plugin has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --cni-conf-dir has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --cni-bin-dir has been deprecated, will be removed along with dockershim.
2021-09-14T00:57:23Z lifecycle-server Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2021-09-14T00:57:23Z lifecycle-server [kubeconfig] Writing "controller-manager.conf" kubeconfig file
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.535111    2114 server.go:440] "Kubelet version" kubeletVersion="v1.21.4"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.549447    2114 dynamic_cafile_content.go:167] Starting client-ca-bundle::/run/config/pki/ca.crt
2021-09-14T00:57:23Z lifecycle-server [kubeconfig] Writing "scheduler.conf" kubeconfig file
2021-09-14T00:57:23Z lifecycle-server [kubelet-start] no supported init system detected, won't make sure the kubelet not running for a short period of time while setting up configuration for it.
2021-09-14T00:57:23Z lifecycle-server [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
2021-09-14T00:57:23Z lifecycle-server [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
2021-09-14T00:57:23Z lifecycle-server [kubelet-start] Starting the kubelet
2021-09-14T00:57:23Z lifecycle-server [kubelet-start] no supported init system detected, won't make sure the kubelet is running properly.
2021-09-14T00:57:23Z lifecycle-server [control-plane] Using manifest folder "/etc/kubernetes/manifests"
2021-09-14T00:57:23Z lifecycle-server [control-plane] Creating static Pod manifest for "kube-apiserver"
2021-09-14T00:57:23Z lifecycle-server [control-plane] Creating static Pod manifest for "kube-controller-manager"
2021-09-14T00:57:23Z lifecycle-server [control-plane] Creating static Pod manifest for "kube-scheduler"
2021-09-14T00:57:23Z lifecycle-server [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
2021-09-14T00:57:23Z lifecycle-server [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
2021-09-14T00:57:23Z lifecycle-server W0914 00:57:23.638395    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.666411    2114 container_manager_linux.go:278] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubepods]
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.666521    2114 container_manager_linux.go:283] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:kubepods CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName:podruntime SystemReservedCgroupName:systemreserved ReservedSystemCPUs: EnforceNodeAllocatable:map[] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>} {Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] E
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.666971    2114 topology_manager.go:120] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.667082    2114 container_manager_linux.go:314] "Initializing Topology Manager" policy="none" scope="container"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.667090    2114 container_manager_linux.go:319] "Creating device plugin manager" devicePluginEnabled=true
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.667776    2114 kubelet.go:307] "Using dockershim is deprecated, please consider using a full-fledged CRI implementation"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.668478    2114 client.go:78] "Connecting to docker on the dockerEndpoint" endpoint="unix:///var/run/docker.sock"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.668517    2114 client.go:97] "Start docker client with request timeout" timeout="2m0s"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.680572    2114 docker_service.go:566] "Hairpin mode is set but kubenet is not enabled, falling back to HairpinVeth" hairpinMode=promiscuous-bridge
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.680608    2114 docker_service.go:242] "Hairpin mode is set" hairpinMode=hairpin-veth
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.695607    2114 hostport_manager.go:72] "The binary conntrack is not installed, this can cause failures in network connection cleanup."
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.695651    2114 hostport_manager.go:72] "The binary conntrack is not installed, this can cause failures in network connection cleanup."
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.702089    2114 docker_service.go:257] "Docker cri networking managed by the network plugin" networkPluginName="cni"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.715683    2114 docker_service.go:264] "Docker Info" dockerInfo=&{ID:D7T5:FM6M:JN3I:ODZO:ZAUW:FEML:INOK:N7WX:6Z3K:STTU:54IG:BLTN Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:58 SystemTime:2021-09-14T00:57:23.703361943Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:4 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSVersion: OSType:linux Archit
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.715723    2114 docker_service.go:277] "Setting cgroupDriver" cgroupDriver="cgroupfs"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.727631    2114 remote_runtime.go:62] parsed scheme: ""
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.727666    2114 remote_runtime.go:62] scheme "" not registered, fallback to default scheme
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728126    2114 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock  <nil> 0 <nil>}] <nil> <nil>}
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728170    2114 clientconn.go:948] ClientConn switching balancer to "pick_first"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728313    2114 remote_image.go:50] parsed scheme: ""
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728325    2114 remote_image.go:50] scheme "" not registered, fallback to default scheme
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728349    2114 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock  <nil> 0 <nil>}] <nil> <nil>}
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.728364    2114 clientconn.go:948] ClientConn switching balancer to "pick_first"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.729536    2114 kubelet.go:404] "Attempting to sync node with API server"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.729571    2114 kubelet.go:272] "Adding static pod path" path="/etc/kubernetes/manifests"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.729752    2114 kubelet.go:283] "Adding apiserver pod source"
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.729802    2114 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
2021-09-14T00:57:23Z lifecycle-server E0914 00:57:23.734146    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://vm.docker.internal:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:23Z lifecycle-server E0914 00:57:23.735650    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://vm.docker.internal:6443/api/v1/nodes?fieldSelector=metadata.name%3Ddocker-desktop&limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:23Z lifecycle-server I0914 00:57:23.752676    2114 kuberuntime_manager.go:222] "Container runtime initialized" containerRuntime="docker" version="20.10.8" apiVersion="1.41.0"
2021-09-14T00:57:24Z lifecycle-server E0914 00:57:24.545816    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://vm.docker.internal:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:24Z lifecycle-server E0914 00:57:24.580809    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://vm.docker.internal:6443/api/v1/nodes?fieldSelector=metadata.name%3Ddocker-desktop&limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:27Z lifecycle-server E0914 00:57:27.117773    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://vm.docker.internal:6443/api/v1/nodes?fieldSelector=metadata.name%3Ddocker-desktop&limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:27Z lifecycle-server time="2021-09-14T00:57:27Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:27Z lifecycle-server E0914 00:57:27.509192    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://vm.docker.internal:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.042969    2114 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
2021-09-14T00:57:29Z lifecycle-server 	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
2021-09-14T00:57:29Z lifecycle-server W0914 00:57:29.044677    2114 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.046711    2114 server.go:1190] "Started kubelet"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.048402    2114 server.go:149] "Starting to listen" address="0.0.0.0" port=10250
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.050247    2114 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.051839    2114 kubelet.go:1306] "Image garbage collection failed once. Stats initialization may not have completed yet" err="failed to get imageFs info: unable to find data in memory cache"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.051958    2114 server.go:405] "Adding debug handlers to kubelet server"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.053474    2114 volume_manager.go:271] "Starting Kubelet Volume Manager"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.055486    2114 desired_state_of_world_populator.go:141] "Desired state populator starts to run"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.059596    2114 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.060259    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://vm.docker.internal:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.060594    2114 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51d2af8e87", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe42c81487, ext:5898549651, loc:(*time.Location)(0x72ca640)}
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.103751    2114 kubelet_network_linux.go:56] "Initialized protocol iptables rules." protocol=IPv4
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.143171    2114 kubelet_network_linux.go:56] "Initialized protocol iptables rules." protocol=IPv6
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.143217    2114 status_manager.go:157] "Starting to sync pod status with apiserver"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.143230    2114 kubelet.go:1846] "Starting kubelet main sync loop"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.143452    2114 kubelet.go:1870] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.146795    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://vm.docker.internal:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.154915    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.184651    2114 cpu_manager.go:199] "Starting CPU manager" policy="none"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.184686    2114 cpu_manager.go:200] "Reconciling" reconcilePeriod="10s"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.184752    2114 state_mem.go:36] "Initialized new in-memory state store"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.186434    2114 policy_none.go:44] "None policy: Start"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.195445    2114 manager.go:600] "Failed to retrieve checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.195613    2114 plugin_manager.go:114] "Starting Kubelet Plugin Manager"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.196089    2114 eviction_manager.go:255] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.199018    2114 kubelet_node_status.go:71] "Attempting to register node" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.201182    2114 kubelet_node_status.go:93] "Unable to register node with API server" err="Post \"https://vm.docker.internal:6443/api/v1/nodes\": dial tcp 192.168.65.4:6443: connect: connection refused" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.246017    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.258147    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.258175    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ceb3704b2330ce9491cc61fc4d1a2529-ca-certs\") pod \"kube-apiserver-docker-desktop\" (UID: \"ceb3704b2330ce9491cc61fc4d1a2529\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.258228    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ceb3704b2330ce9491cc61fc4d1a2529-etc-ca-certificates\") pod \"kube-apiserver-docker-desktop\" (UID: \"ceb3704b2330ce9491cc61fc4d1a2529\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.258248    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ceb3704b2330ce9491cc61fc4d1a2529-k8s-certs\") pod \"kube-apiserver-docker-desktop\" (UID: \"ceb3704b2330ce9491cc61fc4d1a2529\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.258268    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ceb3704b2330ce9491cc61fc4d1a2529-usr-local-share-ca-certificates\") pod \"kube-apiserver-docker-desktop\" (UID: \"ceb3704b2330ce9491cc61fc4d1a2529\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.258290    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ceb3704b2330ce9491cc61fc4d1a2529-usr-share-ca-certificates\") pod \"kube-apiserver-docker-desktop\" (UID: \"ceb3704b2330ce9491cc61fc4d1a2529\") "
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.262933    2114 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.296428    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.336657    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.339278    2114 status_manager.go:566] "Failed to get status for pod" podUID=ceb3704b2330ce9491cc61fc4d1a2529 pod="kube-system/kube-apiserver-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.358838    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.358838    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-kubeconfig\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.358947    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-usr-share-ca-certificates\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.358969    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/b1266d4c79bf4be718d9027c90db001c-kubeconfig\") pod \"kube-scheduler-docker-desktop\" (UID: \"b1266d4c79bf4be718d9027c90db001c\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.359030    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-flexvolume-dir\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.359053    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-k8s-certs\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.359075    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-usr-local-share-ca-certificates\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.359136    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-ca-certs\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.359153    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/4b2d6a96540e71b9ad12f7d031f4d317-etc-ca-certificates\") pod \"kube-controller-manager-docker-desktop\" (UID: \"4b2d6a96540e71b9ad12f7d031f4d317\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.375773    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.387394    2114 status_manager.go:566] "Failed to get status for pod" podUID=4b2d6a96540e71b9ad12f7d031f4d317 pod="kube-system/kube-controller-manager-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.427898    2114 status_manager.go:566] "Failed to get status for pod" podUID=b1266d4c79bf4be718d9027c90db001c pod="kube-system/kube-scheduler-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.451334    2114 kubelet_node_status.go:71] "Attempting to register node" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.454254    2114 kubelet_node_status.go:93] "Unable to register node with API server" err="Post \"https://vm.docker.internal:6443/api/v1/nodes\": dial tcp 192.168.65.4:6443: connect: connection refused" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.459954    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/5d9d97b8d8daed31d6fd5c6d386c29c5-etcd-certs\") pod \"etcd-docker-desktop\" (UID: \"5d9d97b8d8daed31d6fd5c6d386c29c5\") "
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.459999    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/5d9d97b8d8daed31d6fd5c6d386c29c5-etcd-data\") pod \"etcd-docker-desktop\" (UID: \"5d9d97b8d8daed31d6fd5c6d386c29c5\") "
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.460320    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.469473    2114 status_manager.go:566] "Failed to get status for pod" podUID=5d9d97b8d8daed31d6fd5c6d386c29c5 pod="kube-system/etcd-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/etcd-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:29Z lifecycle-server W0914 00:57:29.472107    2114 container.go:586] Failed to update stats for container "/kubepods/kubepods/burstable/pod5d9d97b8d8daed31d6fd5c6d386c29c5": /sys/fs/cgroup/cpuset/kubepods/kubepods/burstable/pod5d9d97b8d8daed31d6fd5c6d386c29c5/cpuset.cpus found to be empty, continuing to push stats
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.561043    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.661831    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.666974    2114 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.763569    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.865094    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.953786    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://vm.docker.internal:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:29Z lifecycle-server I0914 00:57:29.961519    2114 kubelet_node_status.go:71] "Attempting to register node" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.963619    2114 kubelet_node_status.go:93] "Unable to register node with API server" err="Post \"https://vm.docker.internal:6443/api/v1/nodes\": dial tcp 192.168.65.4:6443: connect: connection refused" node="docker-desktop"
2021-09-14T00:57:29Z lifecycle-server E0914 00:57:29.966165    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.067137    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.167311    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server I0914 00:57:30.248781    2114 status_manager.go:566] "Failed to get status for pod" podUID=4b2d6a96540e71b9ad12f7d031f4d317 pod="kube-system/kube-controller-manager-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.267694    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.270662    2114 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://vm.docker.internal:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.65.4:6443: connect: connection refused
2021-09-14T00:57:30Z lifecycle-server I0914 00:57:30.288806    2114 status_manager.go:566] "Failed to get status for pod" podUID=ceb3704b2330ce9491cc61fc4d1a2529 pod="kube-system/kube-apiserver-docker-desktop" error="Get \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-docker-desktop\": dial tcp 192.168.65.4:6443: connect: connection refused"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.369462    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.469940    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.571010    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.672188    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.773229    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server I0914 00:57:30.801469    2114 kubelet_node_status.go:71] "Attempting to register node" node="docker-desktop"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.852783    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:30Z lifecycle-server E0914 00:57:30.953630    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.054683    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.155609    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.256567    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.357070    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.457752    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.558912    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.659881    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.760995    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.862095    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:31Z lifecycle-server E0914 00:57:31.963271    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.064287    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.164539    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.265222    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.366261    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server time="2021-09-14T00:57:32Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.467197    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.567773    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.667875    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.768926    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.869176    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:32Z lifecycle-server E0914 00:57:32.970242    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.071498    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.172870    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.274027    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.375252    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.476357    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.577043    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.677691    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.778037    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.840948    2114 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"docker-desktop\" not found" node="docker-desktop"
2021-09-14T00:57:33Z lifecycle-server E0914 00:57:33.878671    2114 kubelet.go:2291] "Error getting node" err="node \"docker-desktop\" not found"
2021-09-14T00:57:33Z lifecycle-server I0914 00:57:33.910888    2114 kubelet_node_status.go:74] "Successfully registered node" node="docker-desktop"
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.141104    2114 kubelet.go:1683] "Failed creating a mirror pod for" err="pods \"kube-scheduler-docker-desktop\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-scheduler-docker-desktop"
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.523609    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51d2af8e87", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe42c81487, ext:5898549651, loc:(*time.Location)(0x72ca640)}}
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.580865    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9bc2f", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node docker-desktop status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.638791    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9d0b5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node docker-desktop status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af256b
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.697221    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9dc18", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node docker-desktop status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af26218,
2021-09-14T00:57:34Z lifecycle-server I0914 00:57:34.719390    2114 apiserver.go:52] "Watching apiserver"
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.757466    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51db99f3b7", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4bb279b7, ext:60481
2021-09-14T00:57:34Z lifecycle-server I0914 00:57:34.778522    2114 reconciler.go:157] "Reconciler: start to sync state"
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.815493    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9bc2f", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node docker-desktop status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.871839    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9d0b5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node docker-desktop status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af256b
2021-09-14T00:57:34Z lifecycle-server E0914 00:57:34.928033    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9dc18", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node docker-desktop status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af26218,
2021-09-14T00:57:35Z lifecycle-server E0914 00:57:35.105914    2114 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"docker-desktop.16a48a51dad9bc2f", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"docker-desktop", UID:"docker-desktop", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node docker-desktop status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0481abe4af
2021-09-14T00:57:35Z lifecycle-server [apiclient] All control plane components are healthy after 12.007874 seconds
2021-09-14T00:57:35Z lifecycle-server [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
2021-09-14T00:57:35Z lifecycle-server [kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster
2021-09-14T00:57:36Z lifecycle-server [upload-certs] Skipping phase. Please see --upload-certs
2021-09-14T00:57:36Z lifecycle-server [mark-control-plane] Marking the node docker-desktop as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] Using token: abcdef.0123456789abcdef
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
2021-09-14T00:57:36Z lifecycle-server [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
2021-09-14T00:57:36Z lifecycle-server [addons] Applied essential addon: CoreDNS
2021-09-14T00:57:37Z lifecycle-server [addons] Applied essential addon: kube-proxy
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server Your Kubernetes control-plane has initialized successfully!
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server To start using your cluster, you need to run the following as a regular user:
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server   mkdir -p $HOME/.kube
2021-09-14T00:57:37Z lifecycle-server   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
2021-09-14T00:57:37Z lifecycle-server   sudo chown $(id -u):$(id -g) $HOME/.kube/config
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server Alternatively, if you are the root user, you can run:
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server   export KUBECONFIG=/etc/kubernetes/admin.conf
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server You should now deploy a pod network to the cluster.
2021-09-14T00:57:37Z lifecycle-server Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
2021-09-14T00:57:37Z lifecycle-server   https://kubernetes.io/docs/concepts/cluster-administration/addons/
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server You can now join any number of control-plane nodes by copying certificate authorities
2021-09-14T00:57:37Z lifecycle-server and service account keys on each node and then running the following as root:
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server   kubeadm join vm.docker.internal:6443 --token abcdef.0123456789abcdef \
2021-09-14T00:57:37Z lifecycle-server 	--discovery-token-ca-cert-hash sha256:99a99396e9da328764f7028728e9b32e0a682dd55120dba011fb13e37b7e5c8d \
2021-09-14T00:57:37Z lifecycle-server 	--control-plane 
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server Then you can join any number of worker nodes by running the following on each as root:
2021-09-14T00:57:37Z lifecycle-server 
2021-09-14T00:57:37Z lifecycle-server kubeadm join vm.docker.internal:6443 --token abcdef.0123456789abcdef \
2021-09-14T00:57:37Z lifecycle-server 	--discovery-token-ca-cert-hash sha256:99a99396e9da328764f7028728e9b32e0a682dd55120dba011fb13e37b7e5c8d 
2021-09-14T00:57:37Z lifecycle-server + touch /var/lib/kubeadm/.kubeadm-init.sh-finished
2021-09-14T00:57:37Z lifecycle-server + kubeadm version
2021-09-14T00:57:37Z lifecycle-server + cp -r /run/config/pki /var/lib/kubeadm/
2021-09-14T00:57:37Z lifecycle-server time="2021-09-14T00:57:37Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:39Z lifecycle-server E0914 00:57:39.196010    2114 cadvisor_stats_provider.go:415] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/kubepods/burstable/pod5d9d97b8d8daed31d6fd5c6d386c29c5\": RecentStats: unable to find data in memory cache]"
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.488677    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.525748    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-lib-modules\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.526089    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.526205    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-xtables-lock\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.526260    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v2bvf\" (UniqueName: \"kubernetes.io/projected/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-api-access-v2bvf\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.584958    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.598042    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.626954    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.627060    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pwq9m\" (UniqueName: \"kubernetes.io/projected/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-kube-api-access-pwq9m\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.728264    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4kftr\" (UniqueName: \"kubernetes.io/projected/3758e651-a9a5-42a4-bf71-b8d6f44cba96-kube-api-access-4kftr\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") "
2021-09-14T00:57:41Z lifecycle-server I0914 00:57:41.728374    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") "
2021-09-14T00:57:42Z lifecycle-server time="2021-09-14T00:57:42Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:47Z lifecycle-server time="2021-09-14T00:57:47Z" level=info msg="checking kubernetes pods are running: [io.kubernetes.container.name=coredns io.kubernetes.container.name=kube-controller-manager io.kubernetes.container.name=kube-apiserver]"
2021-09-14T00:57:47Z lifecycle-server time="2021-09-14T00:57:47Z" level=info msg="(170c17bb) 65bee725-VMDockerdAPI S->C 51fa1bcd-KubernetesHDL POST /kubernetes/start (1m15.006740301s): OK"
2021-09-14T00:57:47Z lifecycle-server time="2021-09-14T00:57:47Z" level=info msg="(eca54c61) 65bee725-VMDockerdAPI S<-C 51fa1bcd-KubernetesHDL GET /kubernetes/ensure-controllers"
linuxkit/pkg/desktop-host-tools/pkg/client.(*lifecycleClient).EnsureKubeControllers(0xc0006596d0, 0x16a48a561a8e2938, 0x0)
	linuxkit/pkg/desktop-host-tools/pkg/client/client.go:132 +0x5f
common/cmd/com.docker.backend/internal/kubernetes.(*Manager).doStart(0xc000290ba0, 0x5b95001, 0x5b9f528, 0xc0004662a0, 0x43e5b60, 0xc000698120)
	common/cmd/com.docker.backend/internal/kubernetes/bootstrap.go:171 +0x1a2
common/cmd/com.docker.backend/internal/kubernetes.(*Manager).Start(0xc000290ba0, 0x5b9f528, 0xc0004662a0, 0x0, 0x0)
	/Users/administrator/jenkins/workspace/sktop_desktop-build_master-me
2021-09-14T00:57:47Z lifecycle-server + mkdir -p /var/lib/k8s-pvs
2021-09-14T00:57:47Z lifecycle-server + kubectl apply -f /etc/kubeadm/storage-provisioner.yaml
2021-09-14T00:57:47Z lifecycle-server serviceaccount/storage-provisioner created
2021-09-14T00:57:47Z lifecycle-server clusterrolebinding.rbac.authorization.k8s.io/storage-provisioner created
2021-09-14T00:57:47Z lifecycle-server pod/storage-provisioner created
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.705169    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:47Z lifecycle-server storageclass.storage.k8s.io/hostpath created
2021-09-14T00:57:47Z lifecycle-server + kubectl apply -f /etc/kubeadm/vpnkit-controller.yaml
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.786237    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvs\" (UniqueName: \"kubernetes.io/host-path/301592c2-211a-4e96-8931-7d4e62a4e8a4-pvs\") pod \"storage-provisioner\" (UID: \"301592c2-211a-4e96-8931-7d4e62a4e8a4\") "
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.786291    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mf2jc\" (UniqueName: \"kubernetes.io/projected/301592c2-211a-4e96-8931-7d4e62a4e8a4-kube-api-access-mf2jc\") pod \"storage-provisioner\" (UID: \"301592c2-211a-4e96-8931-7d4e62a4e8a4\") "
2021-09-14T00:57:47Z lifecycle-server Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
2021-09-14T00:57:47Z lifecycle-server clusterrole.rbac.authorization.k8s.io/vpnkit-controller created
2021-09-14T00:57:47Z lifecycle-server serviceaccount/vpnkit-controller created
2021-09-14T00:57:47Z lifecycle-server clusterrolebinding.rbac.authorization.k8s.io/vpnkit-controller created
2021-09-14T00:57:47Z lifecycle-server pod/vpnkit-controller created
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.923262    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T00:57:47Z lifecycle-server time="2021-09-14T00:57:47Z" level=info msg="(eca54c61) 65bee725-VMDockerdAPI S->C 51fa1bcd-KubernetesHDL GET /kubernetes/ensure-controllers (479.11043ms): OK"
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.988862    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"api\" (UniqueName: \"kubernetes.io/host-path/c8c190b9-a7e2-496f-8c5b-bc7577b84cd9-api\") pod \"vpnkit-controller\" (UID: \"c8c190b9-a7e2-496f-8c5b-bc7577b84cd9\") "
2021-09-14T00:57:47Z lifecycle-server I0914 00:57:47.988978    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jqzq7\" (UniqueName: \"kubernetes.io/projected/c8c190b9-a7e2-496f-8c5b-bc7577b84cd9-kube-api-access-jqzq7\") pod \"vpnkit-controller\" (UID: \"c8c190b9-a7e2-496f-8c5b-bc7577b84cd9\") "
2021-09-14T00:57:48Z lifecycle-server I0914 00:57:48.472443    2114 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="d3e56cf89911f31d25e534333d2b54b19155f13a49ece723a53aea5916408c70"
2021-09-14T00:57:49Z lifecycle-server E0914 00:57:49.243916    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/c8c190b9-a7e2-496f-8c5b-bc7577b84cd9/etc-hosts with error exit status 1" pod="kube-system/vpnkit-controller"
2021-09-14T00:57:49Z lifecycle-server E0914 00:57:49.252792    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/301592c2-211a-4e96-8931-7d4e62a4e8a4/etc-hosts with error exit status 1" pod="kube-system/storage-provisioner"
2021-09-14T00:57:54Z lifecycle-server I0914 00:57:54.565842    2114 scope.go:111] "RemoveContainer" containerID="553bd87f045c2aa2747b5913686b1c708ac3e048243d56d5c930e81948d2a2eb"
2021-09-14T00:57:55Z lifecycle-server I0914 00:57:55.590273    2114 scope.go:111] "RemoveContainer" containerID="553bd87f045c2aa2747b5913686b1c708ac3e048243d56d5c930e81948d2a2eb"
2021-09-14T00:57:55Z lifecycle-server I0914 00:57:55.592848    2114 scope.go:111] "RemoveContainer" containerID="fe10e0ba4a254c2bc993fd331e374911b9e845450ed93334b61a0e11f052135d"
2021-09-14T00:57:55Z lifecycle-server E0914 00:57:55.593112    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T00:57:56Z lifecycle-server I0914 00:57:56.612352    2114 scope.go:111] "RemoveContainer" containerID="fe10e0ba4a254c2bc993fd331e374911b9e845450ed93334b61a0e11f052135d"
2021-09-14T00:57:56Z lifecycle-server E0914 00:57:56.613115    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T00:58:07Z lifecycle-server I0914 00:58:07.102822    2114 scope.go:111] "RemoveContainer" containerID="fe10e0ba4a254c2bc993fd331e374911b9e845450ed93334b61a0e11f052135d"
2021-09-14T01:02:28Z lifecycle-server W0914 01:02:28.928927    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:07:28Z lifecycle-server W0914 01:07:28.718175    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:12:28Z lifecycle-server W0914 01:12:28.504981    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:15:50Z lifecycle-server I0914 01:15:50.481441    2114 scope.go:111] "RemoveContainer" containerID="fe10e0ba4a254c2bc993fd331e374911b9e845450ed93334b61a0e11f052135d"
2021-09-14T01:15:50Z lifecycle-server I0914 01:15:50.482377    2114 scope.go:111] "RemoveContainer" containerID="9788b2064add25c682a1cf24de82072e95117ed8ff47ee3f975d5e9a3dc913b3"
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.582398    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.719174    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.748054    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mccmv\" (UniqueName: \"kubernetes.io/projected/88d5fc58-c5d6-4935-8645-c6d31d367c2c-kube-api-access-mccmv\") pod \"etcd-58c9bf64b8-j77ln\" (UID: \"88d5fc58-c5d6-4935-8645-c6d31d367c2c\") "
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.748157    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-storage\" (UniqueName: \"kubernetes.io/host-path/88d5fc58-c5d6-4935-8645-c6d31d367c2c-etcd-storage\") pod \"etcd-58c9bf64b8-j77ln\" (UID: \"88d5fc58-c5d6-4935-8645-c6d31d367c2c\") "
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.781409    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.849722    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-g9tfx\" (UniqueName: \"kubernetes.io/projected/91f27352-17ee-495a-bb75-93c778d03b41-kube-api-access-g9tfx\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") "
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.849961    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") "
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.850143    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-disk\" (UniqueName: \"kubernetes.io/host-path/91f27352-17ee-495a-bb75-93c778d03b41-pach-disk\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") "
2021-09-14T01:16:17Z lifecycle-server I0914 01:16:17.951508    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gfxjm\" (UniqueName: \"kubernetes.io/projected/e32ba286-fbce-4a37-ad38-f6b420ab1eaf-kube-api-access-gfxjm\") pod \"dash-866fd997-jdd8k\" (UID: \"e32ba286-fbce-4a37-ad38-f6b420ab1eaf\") "
2021-09-14T01:16:23Z lifecycle-server E0914 01:16:23.365693    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/91f27352-17ee-495a-bb75-93c778d03b41/etc-hosts with error exit status 1" pod="default/pachd-6b565f5ff6-kkdbq"
2021-09-14T01:16:23Z lifecycle-server E0914 01:16:23.396279    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/e32ba286-fbce-4a37-ad38-f6b420ab1eaf/etc-hosts with error exit status 1" pod="default/dash-866fd997-jdd8k"
2021-09-14T01:16:27Z lifecycle-server I0914 01:16:27.012939    2114 kubelet_resources.go:45] "Allocatable" allocatable=map[cpu:{i:{value:4 scale:0} d:{Dec:<nil>} s:4 Format:DecimalSI} ephemeral-storage:{i:{value:56453061334 scale:0} d:{Dec:<nil>} s:56453061334 Format:DecimalSI} hugepages-1Gi:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} hugepages-2Mi:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} memory:{i:{value:1977339904 scale:0} d:{Dec:<nil>} s:1930996Ki Format:BinarySI} pods:{i:{value:110 scale:0} d:{Dec:<nil>} s:110 Format:DecimalSI}]
2021-09-14T01:16:33Z lifecycle-server E0914 01:16:33.429732    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/e32ba286-fbce-4a37-ad38-f6b420ab1eaf/etc-hosts with error exit status 1" pod="default/dash-866fd997-jdd8k"
2021-09-14T01:17:28Z lifecycle-server W0914 01:17:28.301871    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:22:28Z lifecycle-server W0914 01:22:28.082793    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:23:52Z lifecycle-server I0914 01:23:52.112726    2114 scope.go:111] "RemoveContainer" containerID="9788b2064add25c682a1cf24de82072e95117ed8ff47ee3f975d5e9a3dc913b3"
2021-09-14T01:23:52Z lifecycle-server I0914 01:23:52.112905    2114 scope.go:111] "RemoveContainer" containerID="e085d09512ed706eaa07c7dab7210e1170880a85ebf0c96d3eeefb459f56d757"
2021-09-14T01:23:52Z lifecycle-server E0914 01:23:52.116584    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T01:24:03Z lifecycle-server I0914 01:24:02.999900    2114 scope.go:111] "RemoveContainer" containerID="e085d09512ed706eaa07c7dab7210e1170880a85ebf0c96d3eeefb459f56d757"
2021-09-14T01:27:27Z lifecycle-server W0914 01:27:27.863206    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.108583    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.226915    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-disk\" (UniqueName: \"kubernetes.io/host-path/d90aeb27-a883-4030-a274-219922eaad88-pach-disk\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.227081    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4xc5s\" (UniqueName: \"kubernetes.io/projected/d90aeb27-a883-4030-a274-219922eaad88-kube-api-access-4xc5s\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.227525    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.227666    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-bin\" (UniqueName: \"kubernetes.io/empty-dir/d90aeb27-a883-4030-a274-219922eaad88-pach-bin\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.227840    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-tmp\" (UniqueName: \"kubernetes.io/empty-dir/d90aeb27-a883-4030-a274-219922eaad88-pach-tmp\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.228051    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pachyderm-worker\" (UniqueName: \"kubernetes.io/empty-dir/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-worker\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:19Z lifecycle-server I0914 01:30:19.228241    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"docker\" (UniqueName: \"kubernetes.io/host-path/d90aeb27-a883-4030-a274-219922eaad88-docker\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") "
2021-09-14T01:30:27Z lifecycle-server E0914 01:30:27.592531    2114 cadvisor_stats_provider.go:151] "Unable to fetch pod etc hosts stats" err="failed to get stats failed command 'du' ($ nice -n 19 du -x -s -B 1) on path /var/lib/kubelet/pods/d90aeb27-a883-4030-a274-219922eaad88/etc-hosts with error exit status 1" pod="default/pipeline-edges-v1-qhltl"
2021-09-14T01:32:27Z lifecycle-server W0914 01:32:27.646257    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.314831    2114 topology_manager.go:187] "Topology Admit Handler"
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509142    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-bin\" (UniqueName: \"kubernetes.io/empty-dir/6ac33316-b0b2-4142-a0ea-1c7093845beb-pach-bin\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509394    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-disk\" (UniqueName: \"kubernetes.io/host-path/6ac33316-b0b2-4142-a0ea-1c7093845beb-pach-disk\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509529    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pachyderm-worker\" (UniqueName: \"kubernetes.io/empty-dir/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-worker\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509617    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pach-tmp\" (UniqueName: \"kubernetes.io/empty-dir/6ac33316-b0b2-4142-a0ea-1c7093845beb-pach-tmp\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509775    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509840    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"docker\" (UniqueName: \"kubernetes.io/host-path/6ac33316-b0b2-4142-a0ea-1c7093845beb-docker\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:34:24Z lifecycle-server I0914 01:34:24.509959    2114 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7jsnj\" (UniqueName: \"kubernetes.io/projected/6ac33316-b0b2-4142-a0ea-1c7093845beb-kube-api-access-7jsnj\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") "
2021-09-14T01:37:27Z lifecycle-server W0914 01:37:27.422282    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:42:27Z lifecycle-server W0914 01:42:27.201337    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:47:26Z lifecycle-server W0914 01:47:26.978377    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:50:04Z lifecycle-server I0914 01:50:04.356909    2114 scope.go:111] "RemoveContainer" containerID="e085d09512ed706eaa07c7dab7210e1170880a85ebf0c96d3eeefb459f56d757"
2021-09-14T01:50:04Z lifecycle-server I0914 01:50:04.357652    2114 scope.go:111] "RemoveContainer" containerID="c6e1f46adac01d0668b39393cd279f22c0bc8d8b935692361ded2da95b37181b"
2021-09-14T01:52:26Z lifecycle-server W0914 01:52:26.828879    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:57:26Z lifecycle-server W0914 01:57:26.612465    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T01:58:05Z lifecycle-server I0914 01:58:05.671456    2114 scope.go:111] "RemoveContainer" containerID="c6e1f46adac01d0668b39393cd279f22c0bc8d8b935692361ded2da95b37181b"
2021-09-14T01:58:05Z lifecycle-server I0914 01:58:05.672082    2114 scope.go:111] "RemoveContainer" containerID="b5c4a83c8ceb412f199819135a8413030efe82114918b3a29f260a4575d01cac"
2021-09-14T01:58:05Z lifecycle-server E0914 01:58:05.673240    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T01:58:18Z lifecycle-server I0914 01:58:18.574925    2114 scope.go:111] "RemoveContainer" containerID="b5c4a83c8ceb412f199819135a8413030efe82114918b3a29f260a4575d01cac"
2021-09-14T02:02:26Z lifecycle-server W0914 02:02:26.395425    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T02:07:26Z lifecycle-server W0914 02:07:26.179310    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T02:12:25Z lifecycle-server W0914 02:12:25.963308    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T04:15:37Z lifecycle-server E0914 04:15:37.817211    2114 kubelet.go:1870] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 1h59m59.89003004s ago; threshold is 3m0s]"
2021-09-14T04:15:38Z lifecycle-server E0914 04:15:38.447761    2114 kubelet.go:1870] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 2h0m0.52048784s ago; threshold is 3m0s]"
2021-09-14T04:15:38Z lifecycle-server time="2021-09-14T04:15:38Z" level=info msg="iptables -w -t nat -I desktop -p tcp -d 192.168.65.0/24 -j RETURN"
2021-09-14T04:15:38Z lifecycle-server E0914 04:15:38.654858    2114 kubelet.go:1870] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 2h0m0.72756874s ago; threshold is 3m0s]"
2021-09-14T04:15:38Z lifecycle-server W0914 04:15:38.765406    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T04:15:39Z lifecycle-server I0914 04:15:39.774297    2114 scope.go:111] "RemoveContainer" containerID="b5c4a83c8ceb412f199819135a8413030efe82114918b3a29f260a4575d01cac"
2021-09-14T04:15:39Z lifecycle-server I0914 04:15:39.784305    2114 scope.go:111] "RemoveContainer" containerID="8ac2254b1edd95f76bb9db3515318080fcc8d22621f15f13debd5e39c89b9f1c"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.903410    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.903648    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.903986    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.904112    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.904562    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.904630    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907007    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.4036169 +0000 UTC m=+11905.394678029 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907143    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.40711 +0000 UTC m=+11905.398171429 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907229    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.4072097 +0000 UTC m=+11905.398270829 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907283    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.4072612 +0000 UTC m=+11905.398322729 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907364    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.4073425 +0000 UTC m=+11905.398403829 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:39Z lifecycle-server E0914 04:15:39.907433    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:40.4074094 +0000 UTC m=+11905.398470529 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.494469    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.494624    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.494584 +0000 UTC m=+11907.485645829 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.494778    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.494871    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.4948485 +0000 UTC m=+11907.485910329 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528184    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528317    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.5282822 +0000 UTC m=+11907.519344229 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528393    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528478    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.5284414 +0000 UTC m=+11907.519503129 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528549    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528638    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.5286183 +0000 UTC m=+11907.519680529 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528680    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:41Z lifecycle-server E0914 04:15:41.528794    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:15:42.5287752 +0000 UTC m=+11907.519837129 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.519925    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.520074    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.520037 +0000 UTC m=+11910.511098729 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.534308    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.534471    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.5344388 +0000 UTC m=+11910.525500829 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.624619    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.631481    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.634471    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.6292229 +0000 UTC m=+11910.624653029 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.635780    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.648240    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.646815    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.6365475 +0000 UTC m=+11910.627612129 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.659426    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.6583766 +0000 UTC m=+11910.649439329 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:43Z lifecycle-server E0914 04:15:43.669885    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:45.6698441 +0000 UTC m=+11910.660907429 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.540863    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.541138    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.5411072 +0000 UTC m=+11915.532168929 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.540968    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.541477    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.5414514 +0000 UTC m=+11915.532512729 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.642853    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.643118    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.6430899 +0000 UTC m=+11915.634151729 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.642901    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.643407    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.6433821 +0000 UTC m=+11915.634444029 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.743856    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.743954    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.744002    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.7439676 +0000 UTC m=+11915.735029229 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:46Z lifecycle-server E0914 04:15:46.744088    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:50.7440478 +0000 UTC m=+11915.735109129 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:50Z lifecycle-server I0914 04:15:50.538936    2114 scope.go:111] "RemoveContainer" containerID="5bd5e1219e858b454d618e685950af5604b20df4125fd7f67fd9f4806bb5d0a8"
2021-09-14T04:15:50Z lifecycle-server I0914 04:15:50.584869    2114 scope.go:111] "RemoveContainer" containerID="b1828179df2cf0a90b413a654a587268e6b1b907b837336ec45a5443a442ca63"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.621119    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.621267    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.6212123 +0000 UTC m=+11924.612274729 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.624680    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.624802    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.6247746 +0000 UTC m=+11924.615836529 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.723704    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.723986    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.7239148 +0000 UTC m=+11924.714976929 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.724070    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.724402    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.7243693 +0000 UTC m=+11924.715431829 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.827510    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.827544    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.827658    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.8276235 +0000 UTC m=+11924.818685329 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:15:51Z lifecycle-server E0914 04:15:51.827786    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:15:59.8277476 +0000 UTC m=+11924.818810329 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.719361    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.719911    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.719792 +0000 UTC m=+11948.710854729 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.723520    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.724795    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.7240324 +0000 UTC m=+11948.715094929 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.740198    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.740430    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.7403485 +0000 UTC m=+11948.731412329 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.740542    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T04:16:07Z lifecycle-server E0914 04:16:07.740652    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.7406129 +0000 UTC m=+11948.731675129 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:15:50Z lifecycle-server E0914 04:16:07.760245    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T07:15:50Z lifecycle-server E0914 04:16:07.760427    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.7603865 +0000 UTC m=+11948.751447729 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:15:50Z lifecycle-server E0914 04:16:07.760531    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T07:15:50Z lifecycle-server E0914 04:16:07.760970    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 04:16:23.760746 +0000 UTC m=+11948.751823029 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:15:50Z lifecycle-server E0914 07:15:50.538485    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T07:15:50Z lifecycle-server E0914 07:15:50.559262    2114 controller.go:187] failed to update lease, error: Put "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
2021-09-14T07:15:50Z lifecycle-server E0914 07:15:50.597540    2114 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"docker-desktop\": Get \"https://vm.docker.internal:6443/api/v1/nodes/docker-desktop?resourceVersion=0&timeout=30s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
2021-09-14T07:16:06Z lifecycle-server E0914 07:16:06.805422    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T07:16:07Z lifecycle-server I0914 07:16:07.047424    2114 scope.go:111] "RemoveContainer" containerID="b1828179df2cf0a90b413a654a587268e6b1b907b837336ec45a5443a442ca63"
2021-09-14T07:16:07Z lifecycle-server I0914 07:16:07.049256    2114 scope.go:111] "RemoveContainer" containerID="375a7bc5b5fd1247f1d2b878546c472c9488bdd8e0bfc216cf1647d7f045810e"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.478166    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.478166    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.478425    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.4783249 +0000 UTC m=+11981.727556929 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.478756    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.4787241 +0000 UTC m=+11981.727956429 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579108    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579238    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579295    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.579254 +0000 UTC m=+11981.828486029 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579337    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.5793151 +0000 UTC m=+11981.828547129 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579364    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579455    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.5794355 +0000 UTC m=+11981.828667929 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579642    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T07:16:07Z lifecycle-server E0914 07:16:07.579800    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 07:16:39.5797739 +0000 UTC m=+11981.829006229 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T07:16:18Z lifecycle-server E0914 07:16:18.485101    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T11:16:16Z lifecycle-server E0914 11:16:16.338033    2114 controller.go:187] failed to update lease, error: Put "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
2021-09-14T11:16:16Z lifecycle-server E0914 11:16:16.353101    2114 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"vpnkit-controller.16a48a57c3dccfb3", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"5195", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"vpnkit-controller", UID:"c8c190b9-a7e2-496f-8c5b-bc7577b84cd9", APIVersion:"v1", ResourceVersion:"479", FieldPath:"spec.containers{vpnkit-controller}"}, Reason:"Pulled", Message:"Container image \"docker/desktop-vpnkit-controller:v2.0\" already present on machine", Source:v1.EventSource{Compon
2021-09-14T11:16:16Z lifecycle-server I0914 11:16:16.369853    2114 trace.go:205] Trace[11083759]: "Reflector ListAndWatch" name:object-"default"/"pachyderm-storage-secret" (14-Sep-2021 04:15:38.900) (total time: 58954ms):
2021-09-14T11:16:16Z lifecycle-server Trace[11083759]: [58.9543761s] [58.9543761s] END
2021-09-14T11:16:16Z lifecycle-server I0914 11:16:16.372153    2114 trace.go:205] Trace[1069599249]: "Reflector ListAndWatch" name:object-"kube-system"/"coredns" (14-Sep-2021 04:15:38.906) (total time: 58950ms):
2021-09-14T11:16:16Z lifecycle-server Trace[1069599249]: [58.9506856s] [58.9506856s] END
2021-09-14T11:16:16Z lifecycle-server I0914 11:16:16.373599    2114 trace.go:205] Trace[1506680352]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-proxy" (14-Sep-2021 04:15:38.906) (total time: 58949ms):
2021-09-14T11:16:16Z lifecycle-server Trace[1506680352]: [58.9497289s] [58.9497289s] END
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349176    2114 projected.go:293] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349256    2114 projected.go:293] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349539    2114 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349657    2114 projected.go:293] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349905    2114 projected.go:199] Error preparing data for projected volume kube-api-access-4xc5s for pod default/pipeline-edges-v1-qhltl: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm-worker/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350060    2114 projected.go:293] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350074    2114 projected.go:199] Error preparing data for projected volume kube-api-access-mccmv for pod default/etcd-58c9bf64b8-j77ln: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/default/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350094    2114 projected.go:199] Error preparing data for projected volume kube-api-access-gfxjm for pod default/dash-866fd997-jdd8k: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/default/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349905    2114 projected.go:199] Error preparing data for projected volume kube-api-access-g9tfx for pod default/pachd-6b565f5ff6-kkdbq: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350561    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/d90aeb27-a883-4030-a274-219922eaad88-kube-api-access-4xc5s podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8503865 +0000 UTC m=+11964.329713729 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-4xc5s\" (UniqueName: \"kubernetes.io/projected/d90aeb27-a883-4030-a274-219922eaad88-kube-api-access-4xc5s\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm-worker/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349905    2114 projected.go:199] Error preparing data for projected volume kube-api-access-4kftr for pod kube-system/coredns-558bd4d5db-5pzhs: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350639    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/e32ba286-fbce-4a37-ad38-f6b420ab1eaf-kube-api-access-gfxjm podName:e32ba286-fbce-4a37-ad38-f6b420ab1eaf nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8505972 +0000 UTC m=+11964.329923729 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-gfxjm\" (UniqueName: \"kubernetes.io/projected/e32ba286-fbce-4a37-ad38-f6b420ab1eaf-kube-api-access-gfxjm\") pod \"dash-866fd997-jdd8k\" (UID: \"e32ba286-fbce-4a37-ad38-f6b420ab1eaf\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/default/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350840    2114 projected.go:293] Couldn't get configMap default/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350888    2114 projected.go:199] Error preparing data for projected volume kube-api-access-7jsnj for pod default/pipeline-montage-v1-nxc97: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm-worker/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350990    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/6ac33316-b0b2-4142-a0ea-1c7093845beb-kube-api-access-7jsnj podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8509527 +0000 UTC m=+11964.330280429 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-7jsnj\" (UniqueName: \"kubernetes.io/projected/6ac33316-b0b2-4142-a0ea-1c7093845beb-kube-api-access-7jsnj\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm-worker/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.350750    2114 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.351609    2114 projected.go:199] Error preparing data for projected volume kube-api-access-v2bvf for pod kube-system/kube-proxy-f7wwm: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.351916    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-api-access-v2bvf podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8518868 +0000 UTC m=+11964.331214729 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-v2bvf\" (UniqueName: \"kubernetes.io/projected/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-api-access-v2bvf\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.352326    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/91f27352-17ee-495a-bb75-93c778d03b41-kube-api-access-g9tfx podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8523006 +0000 UTC m=+11964.331627129 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-g9tfx\" (UniqueName: \"kubernetes.io/projected/91f27352-17ee-495a-bb75-93c778d03b41-kube-api-access-g9tfx\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/pachyderm/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.352710    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/3758e651-a9a5-42a4-bf71-b8d6f44cba96-kube-api-access-4kftr podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8525899 +0000 UTC m=+11964.331916429 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-4kftr\" (UniqueName: \"kubernetes.io/projected/3758e651-a9a5-42a4-bf71-b8d6f44cba96-kube-api-access-4kftr\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.352346    2114 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.353675    2114 projected.go:199] Error preparing data for projected volume kube-api-access-mf2jc for pod kube-system/storage-provisioner: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/storage-provisioner/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.354006    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/301592c2-211a-4e96-8931-7d4e62a4e8a4-kube-api-access-mf2jc podName:301592c2-211a-4e96-8931-7d4e62a4e8a4 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.853976 +0000 UTC m=+11964.333302729 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-mf2jc\" (UniqueName: \"kubernetes.io/projected/301592c2-211a-4e96-8931-7d4e62a4e8a4-kube-api-access-mf2jc\") pod \"storage-provisioner\" (UID: \"301592c2-211a-4e96-8931-7d4e62a4e8a4\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/storage-provisioner/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.349176    2114 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.355072    2114 projected.go:199] Error preparing data for projected volume kube-api-access-jqzq7 for pod kube-system/vpnkit-controller: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/vpnkit-controller/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.355473    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/88d5fc58-c5d6-4935-8645-c6d31d367c2c-kube-api-access-mccmv podName:88d5fc58-c5d6-4935-8645-c6d31d367c2c nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8553813 +0000 UTC m=+11964.334708629 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-mccmv\" (UniqueName: \"kubernetes.io/projected/88d5fc58-c5d6-4935-8645-c6d31d367c2c-kube-api-access-mccmv\") pod \"etcd-58c9bf64b8-j77ln\" (UID: \"88d5fc58-c5d6-4935-8645-c6d31d367c2c\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/default/serviceaccounts/default/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.354801    2114 controller.go:187] failed to update lease, error: Put "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": read tcp 192.168.65.4:64094->192.168.65.4:6443: use of closed network connection
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.354926    2114 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"docker-desktop\": Get \"https://vm.docker.internal:6443/api/v1/nodes/docker-desktop?timeout=30s\": context deadline exceeded"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.356761    2114 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.356826    2114 projected.go:199] Error preparing data for projected volume kube-api-access-pwq9m for pod kube-system/coredns-558bd4d5db-hn2bq: [failed to fetch token: Post "https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.357066    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-kube-api-access-pwq9m podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.857021 +0000 UTC m=+11964.336347829 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-pwq9m\" (UniqueName: \"kubernetes.io/projected/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-kube-api-access-pwq9m\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server E0914 11:16:17.358235    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/projected/c8c190b9-a7e2-496f-8c5b-bc7577b84cd9-kube-api-access-jqzq7 podName:c8c190b9-a7e2-496f-8c5b-bc7577b84cd9 nodeName:}" failed. No retries permitted until 2021-09-14 11:16:17.8577603 +0000 UTC m=+11964.337087229 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"kube-api-access-jqzq7\" (UniqueName: \"kubernetes.io/projected/c8c190b9-a7e2-496f-8c5b-bc7577b84cd9-kube-api-access-jqzq7\") pod \"vpnkit-controller\" (UID: \"c8c190b9-a7e2-496f-8c5b-bc7577b84cd9\") : [failed to fetch token: Post \"https://vm.docker.internal:6443/api/v1/namespaces/kube-system/serviceaccounts/vpnkit-controller/token\": read tcp 192.168.65.4:57116->192.168.65.4:6443: use of closed network connection, failed to sync configmap cache: timed out waiting for the condition]"
2021-09-14T11:16:17Z lifecycle-server W0914 11:16:17.359305    2114 reflector.go:436] k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: very short watch: k8s.io/client-go/informers/factory.go:134: Unexpected watch close - watch lasted less than a second and no items received
2021-09-14T11:16:19Z lifecycle-server I0914 11:16:19.546948    2114 request.go:668] Waited for 1.1921464s, retries: 2, retry-after: 1s - retry-reason: due to retriable error, error: Get "https://vm.docker.internal:6443/api/v1/pods?fieldSelector=spec.nodeName%!D(MISSING)docker-desktop&resourceVersion=5932": read tcp 192.168.65.4:64096->192.168.65.4:6443: use of closed network connection - request: GET:https://vm.docker.internal:6443/api/v1/pods?fieldSelector=spec.nodeName%!D(MISSING)docker-desktop&resourceVersion=5932
2021-09-14T11:16:21Z lifecycle-server I0914 11:16:21.508785    2114 scope.go:111] "RemoveContainer" containerID="8ac2254b1edd95f76bb9db3515318080fcc8d22621f15f13debd5e39c89b9f1c"
2021-09-14T11:16:21Z lifecycle-server I0914 11:16:21.510728    2114 scope.go:111] "RemoveContainer" containerID="250cfc692944d22635cdf1fca7d949a439b8416f39ebac0b08315f5ea33e3a05"
2021-09-14T11:16:35Z lifecycle-server I0914 11:16:35.480383    2114 scope.go:111] "RemoveContainer" containerID="375a7bc5b5fd1247f1d2b878546c472c9488bdd8e0bfc216cf1647d7f045810e"
2021-09-14T11:16:35Z lifecycle-server I0914 11:16:35.481169    2114 scope.go:111] "RemoveContainer" containerID="80fe58fca3bb0e4a1cf204414d11757acdcf85e494de4d4d57b8a503d2cbe128"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.331452    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.331484    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.331728    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret podName:6ac33316-b0b2-4142-a0ea-1c7093845beb nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.3316438 +0000 UTC m=+12046.810970529 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/6ac33316-b0b2-4142-a0ea-1c7093845beb-pachyderm-storage-secret\") pod \"pipeline-montage-v1-nxc97\" (UID: \"6ac33316-b0b2-4142-a0ea-1c7093845beb\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.331840    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret podName:91f27352-17ee-495a-bb75-93c778d03b41 nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.3318145 +0000 UTC m=+12046.811140929 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/91f27352-17ee-495a-bb75-93c778d03b41-pachyderm-storage-secret\") pod \"pachd-6b565f5ff6-kkdbq\" (UID: \"91f27352-17ee-495a-bb75-93c778d03b41\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433012    2114 configmap.go:200] Couldn't get configMap kube-system/kube-proxy: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433127    2114 secret.go:195] Couldn't get secret default/pachyderm-storage-secret: failed to sync secret cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433200    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy podName:bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.4331655 +0000 UTC m=+12046.912492329 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d-kube-proxy\") pod \"kube-proxy-f7wwm\" (UID: \"bdebd489-75ec-4ab4-9a8e-d3cf6bdd7a8d\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433255    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433281    2114 configmap.go:200] Couldn't get configMap kube-system/coredns: failed to sync configmap cache: timed out waiting for the condition
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433281    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret podName:d90aeb27-a883-4030-a274-219922eaad88 nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.4332557 +0000 UTC m=+12046.912582929 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"pachyderm-storage-secret\" (UniqueName: \"kubernetes.io/secret/d90aeb27-a883-4030-a274-219922eaad88-pachyderm-storage-secret\") pod \"pipeline-edges-v1-qhltl\" (UID: \"d90aeb27-a883-4030-a274-219922eaad88\") : failed to sync secret cache: timed out waiting for the condition"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433505    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume podName:8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5 nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.433413 +0000 UTC m=+12046.912759029 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5-config-volume\") pod \"coredns-558bd4d5db-hn2bq\" (UID: \"8f7aa6dd-3185-45e3-a3fa-cb0069fda1b5\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T11:16:36Z lifecycle-server E0914 11:16:36.433570    2114 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume podName:3758e651-a9a5-42a4-bf71-b8d6f44cba96 nodeName:}" failed. No retries permitted until 2021-09-14 11:17:40.4335497 +0000 UTC m=+12046.912876129 (durationBeforeRetry 1m4s). Error: "MountVolume.SetUp failed for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3758e651-a9a5-42a4-bf71-b8d6f44cba96-config-volume\") pod \"coredns-558bd4d5db-5pzhs\" (UID: \"3758e651-a9a5-42a4-bf71-b8d6f44cba96\") : failed to sync configmap cache: timed out waiting for the condition"
2021-09-14T11:16:38Z lifecycle-server I0914 11:16:38.436512    2114 trace.go:205] Trace[1801688365]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (14-Sep-2021 04:15:40.024) (total time: 79899ms):
2021-09-14T11:16:38Z lifecycle-server Trace[1801688365]: ---"Objects listed" 79899ms (11:16:00.435)
2021-09-14T11:16:38Z lifecycle-server Trace[1801688365]: [1m19.899383s] [1m19.899383s] END
2021-09-14T11:16:38Z lifecycle-server I0914 11:16:38.521511    2114 trace.go:205] Trace[311979613]: "Reflector ListAndWatch" name:k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:66 (14-Sep-2021 04:15:40.536) (total time: 79473ms):
2021-09-14T11:16:38Z lifecycle-server Trace[311979613]: ---"Objects listed" 79473ms (11:16:00.521)
2021-09-14T11:16:38Z lifecycle-server Trace[311979613]: [1m19.4734191s] [1m19.4734191s] END
2021-09-14T11:16:38Z lifecycle-server I0914 11:16:38.554003    2114 trace.go:205] Trace[1000007775]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (14-Sep-2021 04:15:40.799) (total time: 79243ms):
2021-09-14T11:16:38Z lifecycle-server Trace[1000007775]: ---"Objects listed" 79243ms (11:16:00.553)
2021-09-14T11:16:38Z lifecycle-server Trace[1000007775]: [1m19.2431601s] [1m19.2431601s] END
2021-09-14T11:16:38Z lifecycle-server I0914 11:16:38.597968    2114 trace.go:205] Trace[1783054529]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:134 (14-Sep-2021 11:16:18.554) (total time: 20043ms):
2021-09-14T11:16:38Z lifecycle-server Trace[1783054529]: ---"Objects listed" 20043ms (11:16:00.597)
2021-09-14T11:16:38Z lifecycle-server Trace[1783054529]: [20.0432276s] [20.0432276s] END
2021-09-14T11:16:44Z lifecycle-server E0914 11:16:44.722188    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:30:32Z lifecycle-server W0914 13:30:32.450697    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T13:38:26Z lifecycle-server E0914 13:38:26.069747    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:39:19Z lifecycle-server E0914 13:39:19.372539    2114 controller.go:187] failed to update lease, error: Put "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
2021-09-14T13:39:19Z lifecycle-server E0914 13:39:19.554729    2114 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pachd-6b565f5ff6-kkdbq.16a49ef76d0cf508", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"6605", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"pachd-6b565f5ff6-kkdbq", UID:"91f27352-17ee-495a-bb75-93c778d03b41", APIVersion:"v1", ResourceVersion:"1941", FieldPath:"spec.containers{pachd}"}, Reason:"Unhealthy", Message:"Readiness probe failed: ", Source:v1.EventSource{Component:"kubelet", Host:"docker-desktop"}, FirstTimestamp:v1.Time{Time
2021-09-14T13:39:19Z lifecycle-server E0914 13:39:19.626578    2114 kubelet.go:1870] "Skipping pod synchronization" err="container runtime is down"
2021-09-14T13:39:19Z lifecycle-server E0914 13:39:19.772257    2114 kubelet.go:1870] "Skipping pod synchronization" err="container runtime is down"
2021-09-14T13:39:20Z lifecycle-server E0914 13:39:20.297171    2114 cadvisor_stats_provider.go:415] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/kubepods/burstable/podb1266d4c79bf4be718d9027c90db001c/5a0c1a86d3936eda3b2ccfae7e532ec0df38f72b58d0d5e217223616858a337b\": RecentStats: unable to find data in memory cache]"
2021-09-14T13:39:21Z lifecycle-server I0914 13:39:21.118516    2114 scope.go:111] "RemoveContainer" containerID="5bd5e1219e858b454d618e685950af5604b20df4125fd7f67fd9f4806bb5d0a8"
2021-09-14T13:39:21Z lifecycle-server I0914 13:39:21.118683    2114 scope.go:111] "RemoveContainer" containerID="5a0c1a86d3936eda3b2ccfae7e532ec0df38f72b58d0d5e217223616858a337b"
2021-09-14T13:39:21Z lifecycle-server I0914 13:39:21.167801    2114 scope.go:111] "RemoveContainer" containerID="07e1b182337299622b023d7ced56601b4a9d699e41e44b912e3ccbbf26d77b69"
2021-09-14T13:39:21Z lifecycle-server I0914 13:39:21.208724    2114 scope.go:111] "RemoveContainer" containerID="80fe58fca3bb0e4a1cf204414d11757acdcf85e494de4d4d57b8a503d2cbe128"
2021-09-14T13:41:19Z lifecycle-server E0914 13:41:19.319561    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = operation timeout: context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:41:38Z lifecycle-server E0914 13:41:38.963206    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:41:39Z lifecycle-server E0914 13:41:39.969498    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:41:40Z lifecycle-server E0914 13:41:40.976907    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:00Z lifecycle-server E0914 13:42:00.572586    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:30Z lifecycle-server E0914 13:42:30.541760    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:39Z lifecycle-server E0914 13:42:39.535212    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = container not running (a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977)" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:39Z lifecycle-server E0914 13:42:39.537931    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = container not running (a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977)" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:39Z lifecycle-server E0914 13:42:39.540584    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = container not running (a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977)" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977" cmd=[/pachd --readiness]
2021-09-14T13:42:39Z lifecycle-server I0914 13:42:39.707129    2114 scope.go:111] "RemoveContainer" containerID="a4b47a8eef0b0a1fd7cd1a7b414c3e6a5ad687a051250dd96a88addffa56b977"
2021-09-14T13:42:39Z lifecycle-server I0914 13:42:39.713862    2114 kubelet_resources.go:45] "Allocatable" allocatable=map[cpu:{i:{value:4 scale:0} d:{Dec:<nil>} s:4 Format:DecimalSI} ephemeral-storage:{i:{value:56453061334 scale:0} d:{Dec:<nil>} s:56453061334 Format:DecimalSI} hugepages-1Gi:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} hugepages-2Mi:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} memory:{i:{value:1977339904 scale:0} d:{Dec:<nil>} s:1930996Ki Format:BinarySI} pods:{i:{value:110 scale:0} d:{Dec:<nil>} s:110 Format:DecimalSI}]
2021-09-14T13:42:41Z lifecycle-server E0914 13:42:41.766399    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T13:42:50Z lifecycle-server W0914 13:42:50.120410    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T13:47:49Z lifecycle-server W0914 13:47:49.764687    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T13:52:49Z lifecycle-server W0914 13:52:49.412656    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T13:55:17Z lifecycle-server I0914 13:55:17.797534    2114 scope.go:111] "RemoveContainer" containerID="250cfc692944d22635cdf1fca7d949a439b8416f39ebac0b08315f5ea33e3a05"
2021-09-14T13:55:17Z lifecycle-server I0914 13:55:17.798689    2114 scope.go:111] "RemoveContainer" containerID="478b895c82cd3a996902c7e3a3af2a38b5c6ad6c50b02865d2bf0d4fb2ce9473"
2021-09-14T13:57:49Z lifecycle-server W0914 13:57:49.064153    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:02:48Z lifecycle-server W0914 14:02:48.713780    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:03:18Z lifecycle-server I0914 14:03:18.896225    2114 scope.go:111] "RemoveContainer" containerID="478b895c82cd3a996902c7e3a3af2a38b5c6ad6c50b02865d2bf0d4fb2ce9473"
2021-09-14T14:03:19Z lifecycle-server I0914 14:03:19.269605    2114 scope.go:111] "RemoveContainer" containerID="32c336737583513337a87b3b6a4778f236c0f0f1934a8c35091f89d6a6972450"
2021-09-14T14:03:19Z lifecycle-server E0914 14:03:19.270845    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T14:03:34Z lifecycle-server I0914 14:03:34.684110    2114 scope.go:111] "RemoveContainer" containerID="32c336737583513337a87b3b6a4778f236c0f0f1934a8c35091f89d6a6972450"
2021-09-14T14:07:48Z lifecycle-server W0914 14:07:48.362274    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:11:36Z lifecycle-server I0914 14:11:36.208939    2114 scope.go:111] "RemoveContainer" containerID="32c336737583513337a87b3b6a4778f236c0f0f1934a8c35091f89d6a6972450"
2021-09-14T14:11:36Z lifecycle-server I0914 14:11:36.209607    2114 scope.go:111] "RemoveContainer" containerID="40c3742a4493f14a74544b236b72cb2d3564426d06ba212f9f5c71ee964f19b4"
2021-09-14T14:11:36Z lifecycle-server E0914 14:11:36.209972    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 20s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T14:11:47Z lifecycle-server I0914 14:11:47.088983    2114 scope.go:111] "RemoveContainer" containerID="40c3742a4493f14a74544b236b72cb2d3564426d06ba212f9f5c71ee964f19b4"
2021-09-14T14:11:47Z lifecycle-server E0914 14:11:47.089505    2114 pod_workers.go:190] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vpnkit-controller\" with CrashLoopBackOff: \"back-off 20s restarting failed container=vpnkit-controller pod=vpnkit-controller_kube-system(c8c190b9-a7e2-496f-8c5b-bc7577b84cd9)\"" pod="kube-system/vpnkit-controller" podUID=c8c190b9-a7e2-496f-8c5b-bc7577b84cd9
2021-09-14T14:12:00Z lifecycle-server I0914 14:12:00.088788    2114 scope.go:111] "RemoveContainer" containerID="40c3742a4493f14a74544b236b72cb2d3564426d06ba212f9f5c71ee964f19b4"
2021-09-14T14:12:48Z lifecycle-server W0914 14:12:48.015381    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:17:47Z lifecycle-server W0914 14:17:47.666506    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:22:21Z lifecycle-server E0914 14:22:20.970171    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:22:48Z lifecycle-server W0914 14:22:48.505211    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:53:14Z lifecycle-server E0914 14:53:14.903714    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:53:24Z lifecycle-server E0914 14:53:24.520214    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:53:37Z lifecycle-server E0914 14:53:37.774621    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:53:42Z lifecycle-server E0914 14:53:42.374459    2114 controller.go:187] failed to update lease, error: Put "https://vm.docker.internal:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/docker-desktop?timeout=30s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
2021-09-14T14:53:46Z lifecycle-server E0914 14:53:46.121677    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:53:52Z lifecycle-server E0914 14:53:52.524903    2114 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"docker-desktop\": Get \"https://vm.docker.internal:6443/api/v1/nodes/docker-desktop?resourceVersion=0&timeout=30s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
2021-09-14T14:53:59Z lifecycle-server E0914 14:53:59.058570    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:54:02Z lifecycle-server I0914 14:54:02.812099    2114 scope.go:111] "RemoveContainer" containerID="8e2d8d92a9473c56b53c768902c758bc52cf3e3a1414553b37850f4779a3e587"
2021-09-14T14:54:02Z lifecycle-server I0914 14:54:02.833060    2114 scope.go:111] "RemoveContainer" containerID="5a0c1a86d3936eda3b2ccfae7e532ec0df38f72b58d0d5e217223616858a337b"
2021-09-14T14:54:02Z lifecycle-server I0914 14:54:02.851009    2114 scope.go:111] "RemoveContainer" containerID="6aa18fa0a8a00821b84bf124d9d37630f9d2a39eaac42bf756022d63cc5954f6"
2021-09-14T14:54:03Z lifecycle-server I0914 14:54:03.364336    2114 scope.go:111] "RemoveContainer" containerID="07e1b182337299622b023d7ced56601b4a9d699e41e44b912e3ccbbf26d77b69"
2021-09-14T14:54:04Z lifecycle-server E0914 14:54:04.866568    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:54:07Z lifecycle-server I0914 14:54:07.639733    2114 trace.go:205] Trace[1768995967]: "Reflector ListAndWatch" name:object-"kube-system"/"kube-proxy" (14-Sep-2021 14:53:53.734) (total time: 13905ms):
2021-09-14T14:54:07Z lifecycle-server Trace[1768995967]: ---"Objects listed" 13904ms (14:54:00.638)
2021-09-14T14:54:07Z lifecycle-server Trace[1768995967]: [13.9050642s] [13.9050642s] END
2021-09-14T14:54:08Z lifecycle-server I0914 14:54:08.907525    2114 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="07e1b182337299622b023d7ced56601b4a9d699e41e44b912e3ccbbf26d77b69"
2021-09-14T14:55:53Z lifecycle-server E0914 14:55:53.937856    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:03Z lifecycle-server E0914 14:56:03.940698    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:13Z lifecycle-server E0914 14:56:13.902605    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:23Z lifecycle-server E0914 14:56:23.899535    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:33Z lifecycle-server E0914 14:56:33.902399    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:43Z lifecycle-server E0914 14:56:43.874729    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:56:53Z lifecycle-server E0914 14:56:53.947671    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:03Z lifecycle-server E0914 14:57:03.867166    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:13Z lifecycle-server E0914 14:57:13.830847    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:23Z lifecycle-server E0914 14:57:23.833449    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:33Z lifecycle-server E0914 14:57:33.832967    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:43Z lifecycle-server W0914 14:57:43.416399    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-14T14:57:43Z lifecycle-server E0914 14:57:43.799738    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:57:53Z lifecycle-server E0914 14:57:53.795199    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:03Z lifecycle-server E0914 14:58:03.794579    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:13Z lifecycle-server E0914 14:58:13.775180    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:23Z lifecycle-server E0914 14:58:23.764675    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:33Z lifecycle-server E0914 14:58:33.764650    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:43Z lifecycle-server E0914 14:58:43.726891    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:58:53Z lifecycle-server E0914 14:58:53.729934    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:59:03Z lifecycle-server E0914 14:59:03.724125    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:59:13Z lifecycle-server E0914 14:59:13.691690    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:59:23Z lifecycle-server E0914 14:59:23.693696    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:59:33Z lifecycle-server E0914 14:59:33.697663    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-14T14:59:39Z lifecycle-server I0914 14:59:39.239314    2114 scope.go:111] "RemoveContainer" containerID="40c3742a4493f14a74544b236b72cb2d3564426d06ba212f9f5c71ee964f19b4"
2021-09-14T14:59:39Z lifecycle-server I0914 14:59:39.242569    2114 scope.go:111] "RemoveContainer" containerID="e52228fa7248486f3267dcc337bab37f65ea4ffaa64e755bf405e7496222fedb"
2021-09-14T14:59:43Z lifecycle-server E0914 14:59:43.665058    2114 remote_runtime.go:394] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="fe68686a2818b0dbcfce17b1afca802b81193b9c591d1628f7c80ac651b59d7a" cmd=[/pachd --readiness]
2021-09-15T01:17:39Z lifecycle-server W0915 01:17:39.572140    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
2021-09-15T01:22:38Z lifecycle-server I0915 01:22:38.890808    2114 scope.go:111] "RemoveContainer" containerID="e8da849e946176fd15951249eab8c291f42effe9c7a1ba316713e53855b4cb41"
2021-09-15T01:22:38Z lifecycle-server I0915 01:22:38.891991    2114 scope.go:111] "RemoveContainer" containerID="e52228fa7248486f3267dcc337bab37f65ea4ffaa64e755bf405e7496222fedb"
2021-09-15T01:22:39Z lifecycle-server W0915 01:22:39.219955    2114 sysinfo.go:203] Nodes topology is not available, providing CPU topology
